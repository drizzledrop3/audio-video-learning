# 音视频入门科普





## 音视频背景

ISO：国际标准化组织，官方网址：[ISO - International Organization for Standardization](https://www.iso.org/home.html)。很多音视频协议都可以从这里找到。

**MPEG**：ISO与IEC下属的针对运动图像与语音压缩制定国际标准的组织，全称为运动图像专家组(Moving Picture Experts Group)，官网网址：[https://mpeg.chiariglione.org](https://mpeg.chiariglione.org/)。

IETF：互联网工程任务组(Internet Engineering Task Force)，官方网址：[https://www.ietf.org](https://www.ietf.org/)。常见的网络协议、多媒体通信协议的地址：https://tools.ietf.org/html/。

### MPEG发展历史

| 名称   | 编号         | 年份 | 描述                                            |
| ------ | ------------ | ---- | ----------------------------------------------- |
| MPEG1  | ISO/IEC11172 | 1992 | 用于存储语音的编码，MPEG1 audio layer3，简称mp3 |
| MPEG2  | ISO/IEC13818 | 1994 | 用于数字电视、图像通信的编码                    |
| MPEG4  | ISO/IEC14496 | 1998 | 用于视频电话、家庭影音的编码                    |
| MPEG7  | /            | 1999 | 生成一种用来描述多媒体内容的标准，MPEG7=1+2+4   |
| MPEG21 | /            | 1999 | 多媒体框架                                      |

MPEG（Moving Picture Experts Group）是一系列的视频和音频压缩标准，由国际电信联盟（ITU）和国际电工委员会（IEC）联合制定。MPEG 标准定义了一系列的压缩方法，其中包括 MPEG-1、MPEG-2、MPEG-4 等。每个标准都针对不同的应用领域和需求。

下面是 MPEG-1、MPEG-2 和 MPEG-4 的简要说明：

1. **MPEG-1：**
   - **发布年份：** 1993
   - **应用领域：** 主要用于 VCD（Video CD）和 CD-i（Compact Disc Interactive）等低分辨率、低比特率的视频应用。
   - **特点：** MPEG-1 最初设计用于数字媒体存储，支持相对低分辨率（352x240或352x288）的视频和单声道音频。它采用基于帧的压缩，并使用 DCT（离散余弦变换）进行图像压缩。MPEG-1 视频通常以 1.5 Mbps 的比特率进行编码。

2. **MPEG-2：**
   - **发布年份：** 1995
   - **应用领域：** 适用于广泛的应用领域，包括 DVD、数字电视、数字广播等。
   - **特点：** MPEG-2 是 MPEG-1 的扩展，支持高分辨率、多通道音频、多角度等特性。它适用于广播和存储系统，具有更高的比特率和更好的图像质量。MPEG-2 支持多种分辨率，包括标清和高清。它也采用 DCT 进行图像压缩。

3. **MPEG-4：**
   - **发布年份：** 1998
   - **应用领域：** 适用于广泛的应用领域，包括网络流媒体、移动通信、视频会议等。
   - **特点：** MPEG-4 是一种更为灵活和复杂的标准，支持更高级的视频和音频编解码技术。它引入了基于物体的编码，允许对视频中的特定对象进行编码和解码。MPEG-4 还支持更低的比特率，更好的视频质量，以及多媒体对象的交互性。其中，H.264/AVC 和 H.265/HEVC 是 MPEG-4 标准的一部分，分别是 MPEG-4 Part 10 和 MPEG-H Part 2 标准的一部分，广泛用于高效的视频压缩。

这些 MPEG 标准在不同的应用场景中发挥着重要作用，为数字媒体的存储、传输和展示提供了关键的技术支持。



## 视频

### 码率

码率，又叫比特率，单位时间内传输的数据量，单位一般为kbps(千位每秒)。需要注意的是，这里b代表bit，而不是byte。
计算公式：

- 平均码率(kbps)=文件大小(kb) * 8 / 时间(s)
- 动态码率(kbps)=每秒传输数据量(kb) * 8



恒定码率：CBR，码率稳定可控，带宽要求不高，图像变化量比较大时**方块效应比较明显**。

动态码率：VBR，码率波动较大，带宽要求较高，图像变化量比较大时方块效应有所改善。发生网络抖动时，比较容易丢包，需要重传，或者FEC前向纠错，从而**带来延时**。



### 分辨率

分辨率又称为解析度，分辨率越高，像素越多，图像越清晰。

视频分辨率：又称为图像分辨率，由视频的宽高组成，表示形式**宽x高**，常见的视频分辨率有**480P、720P、1080P、2K(2048x1080/2160x1440)、4K(4096x2160/3840x2160)**，具体如下表1所示。

屏幕分辨率：又称为显示分辨率，描述屏幕分辨率的单位是**ppi(pixel per inch，每英寸的像素数)**。因此显然，屏幕分辨率在视频分辨率基础上增加了对实际屏幕的限定。

位分辨率：又称为位深(BitDepth)，每个像素点存储信息的位数。常见的有：8位、16位、24位、32位色彩。Android的Bitmap常见的有ALPHA_8、RGB_565、ARGB_4444、ARGB_8888。

| 显示模式 | 水平像素x垂直像素 | 宽高比 |
| -------- | ----------------- | ------ |
| QCIF     | 176x144           | 11:9   |
| QVGA     | 320x240           | 4:3    |
| CIF      | 352x288           | 11:9   |
| nHD      | 640x360           | 16:9   |
| VGA      | 640x480           | 4:3    |
| HD       | 1280x720          | 16:9   |
| Full HD  | 1920x1080         | 16:9   |
| 2K(FHD+) | 2048x1080         | 17:9   |
| 4K(UHD)  | 3840x2160         | 16:9   |



#### DPI与PPI

##### 概念问题

不论是DPI还是PPI，实际都是一种**换算的概念**，即将图片承载的信息换算为现实中的图片（即人眼能实际看到的图像）。DPI和PPI的区别在于换算的途径不同，DPI面向的是**印刷受体**，而PPI面向的是**荧幕**。



##### PPI

**PPI**是英文`Pixels Per Inch`的缩写，意为**像素每英寸**。英寸是常用的长度单位，大约相当于2.54厘米。而**像素是专用于荧幕的概念**，指的是**荧幕可以解析的最小的点**。因此，PPI值得是像素在荧幕上的密度，PPI越高图像就越清晰。
举例来说，如果电脑屏幕是2K分辨率，即1920×1080像素，它的图像宽为1920像素。而如果这个电脑屏幕的物理宽度是19.2英寸，电脑屏幕是**分辨率就是1920/19.2=100PPI**。



##### DPI

**DPI**是英文`Dots Per Inch`的缩写，意为**点每英寸**。英寸还是那个英寸，但是点的意义有很多。一般来讲，你可以把Dot理解为**取样点**，即**物理设备可以解析的最小单位**。在印刷时，它就可以作为**印刷网点**，而在鼠标等电子设备上，可以理解为**最小操作阈值**（即设备会把多么远的两个点当作一个点来处理）。
我们仍然拿1920×1080像素的图片来举例子，如果印刷设备的解析能力刚好是100DPI，而且你要印制的纸张尺寸刚好是19.2英寸，那么印刷设备就可以刚好把一个像素作为一个取样点，印刷完成后图片的**保真度是百分之百**（也就是图片所有的视觉信息都被印刷出来了）。在大多数情况下，这几个数值都不那么整好，因此保真度会产生损失。



##### 图片内置的DPI和PPI

图片在计算机（或其他设备）里是一系列代表视觉信息的数据，它的单位是像素。因此，真正能定义图片尺寸的是分辨率，比如前面提到的1920×1080像素。

而很多格式的图片会内置DPI或PPI这个属性，它的唯一作用是作为图形处理软件的参考值。比如，一张图片的PPI是300，那么置入Illustrator的时候就会直接是300PPI下的尺寸。DPI是完全相同的道理。换言之，不论图片的DPI和PPI如何变化，如果**分辨率不变**，那么图片承载的信息量就不会变化，在**实际意义上图片的“大小”都是相同的**。



##### 设备的DPI和PPI

我们之前提到了印刷设备的解析能力这个问题。其实**每个荧幕和每个印刷设备都有自己PPI或DPI参数**。我们拿荧幕来说，荧幕的PPI就决定了荧幕的解析能力（注意，并非最大解析能力，而是**绝对解析能力**）。

如你把某个图片的尺寸在屏幕上放大缩小，它的物理尺寸在改变，因此对信息量来说PPI也在改变。然而，荧幕会按照它自己的PPI显示能力来重新解析这张图片，最终形成你肉眼看到的结果。这个过程，我们可以称为“**栅格处理**”。



##### 栅格处理

屏幕的栅格处理指的是将屏幕上的图像、文本等内容按照一个规定的栅格进行处理的过程。在计算机图形学和计算机视觉中，屏幕通常被划分为一个个的像素，每个像素可以看作是屏幕上的一个小方格，这些小方格构成了屏幕的栅格。在这个栅格上，可以对每个像素的颜色、亮度等属性进行编码和存储，从而呈现出图像、文字等视觉内容。栅格处理常常涉及到像素的放大、缩小、旋转、变形等操作，以及像素颜色的调整、滤波、插值等处理，以达到预期的视觉效果。
实际就是将图片**在物理尺寸不变**的情况下，**对DPI或PPI进行调整**，图片的信息量会受到影响。图像为何需要进行栅格处理？因为任何设备都有固定的解析能力，比如很多荧幕的解析能力是72PPI，这时一张全屏后（即在屏幕的物理尺寸下）从信息量上来说有300PPI的图片显然超过了荧幕的解析能力，因此对荧幕来说这么大的图片是没必要的，把图片在荧幕的物理尺寸下处理为72PPI就刚好了。
**几乎所有的栅格都是有损处理，除了某些算法中的整数倍放大。**



### **视频帧**

帧是视频的一个基本概念，表示一幅画面，一段视频由许多帧组成。



### **帧率**

视频帧率：**显示帧数的量度**，单位为**每秒显示帧数**(**FPS**，全称为`Frame Per Second`)。一般视频帧率为24fps，**P制**(PAL，德国提出，中国、印度、巴基斯坦等国家使用)为**25fps**，也就是每帧显示40ms，**N制**(NTSC，美国标准委员会提出，美国、日本、韩国等国家使用)为**30fps**。有些超高帧率的视频达到**60fps**。

常见帧率：

29.97 f/s : 1s 30000/1001帧

24 f/s  或 25 f/s ：1s 24 或 25 帧， 一般的电视/电影帧率

30 f/s  或 60 f/s ：1s 30 或 60 帧， 游戏中30帧可以接受，60帧会感觉非常流畅

一般来讲，85 f/s 以上人眼基本上无法察觉出过度画面，所以过高的帧率在普通游戏视频里没有太大意义。

显示帧率：以帧为单位的位图图像连续出现在显示器的频率，也称为**刷新速率**。通常来说，**人眼能够感知到的最低帧率是每秒 24 帧**，也就是每帧的时间大约是 **42 ms**。如果帧率低于这个值，就会出现卡顿和延迟的感觉，画面也会变得不流畅。在某些高速运动场景中，甚至需要更高的帧率才能保证画面的流畅度和清晰度，比如说游戏或者体育直播等领域，此时一般会追求更高的帧率来提供更好的用户体验。
Android设备刷新率一般为60Hz，也就是帧率为60fps，**每帧为16ms**，由于交互原因，**超过16ms能给人的肉眼带来延迟卡顿的感觉**。做性能优化方面，也就是保证从测量、布局、绘制、上传指令、与GPU交换缓冲区等一系列动作在16ms完成。Android11支持120Hz的更高帧率，一般为对帧率要求极高的应用场景，比如互动游戏。



### **像素格式**

像素格式：像素色彩分量的排列，由每个像素使用的总位数以及各分量的位数决定。图像的像素格式一般是RGBA四个分量通道各占8bits，组成一个32位的像素。其中R代表Red、G代表Green、B代表Blue、A代表Alpha。但是，视频压缩存储的像素格式不是RGBA，而是YUV，其中Y代表亮度(Luma)，U代表色度(Chroma)，V代表对比度(Contrast)。

#### 像素通道

在数字图像中，每个像素都是由数值表示的，这些数值代表了图像上对应点的颜色和亮度信息。因此，图像的本质是由一个个像素点组成的。每个像素点都包含了颜色和亮度等信息。

然而，对于彩色图像而言，每个像素点不仅仅包含了一个颜色信息，而是包含了红、绿、蓝三种颜色信息的组合。这种组合被称为“**颜色通道**”，而包含多个颜色通道的图像就是多通道图像。例如，RGB图像就是一种三通道图像，其中每个像素包含了红色通道、绿色通道和蓝色通道的信息。还有其他的多通道图像格式，例如RGBA图像、CMYK图像等。

多通道图像能够提供更加丰富的颜色信息，因此在一些颜色重要的应用场景下得到了广泛的应用，例如数码相机、电影、电视、游戏等。

> 因此所谓像素通道，并不是说一张图是有多层叠加出来的，而是每个像素点包含多个信息位。





### **画质**

画质：画面质量，由清晰度、锐度、解析度、色彩纯度、色彩平衡等指标构成。(偏主观的概念)

清晰度：指图像细节纹理及其边界的清晰程度。

锐度：反应图像平面清晰程度，以及图像边缘的锐利程度。

解析度：指像素点的数量，与分辨率对应，分辨率越高，解析度越高。

色彩纯度：指色彩的鲜艳程度。所有色彩都是三原色组成，其他颜色都是三原色混合而成，理论上可以混合出256种颜色。原色的纯度最高。**色彩纯度是指原色在色彩中的百分比**。

色彩平衡：用来控制图像的色彩分布，使得图像整体达到色彩平衡。



#### 关于三原色

三原色是指无法再被分解，也不能由其他颜色混合而成的三种颜色。根据应用场景的不同，三原色可以分为两类：叠加型的三原色和消減型的三原色。

**叠加型**的三原色是**红色、绿色和蓝色**，也称为**RGB**。它们用于电视机、投影仪等显示设备。这三种颜色可以混合产生其他颜色，例如红色与绿色混合可以产生黄色或橙色，绿色与蓝色混合可以产生青色，蓝色与红色混合可以产生紫色或品红色 。

**消減型**的三原色是**桃红色、黄色和青色**，也称为**CMY**。它们用于书本、杂志等的印刷。





### **色域与HDR**

色域：指某种表色模式所能表达的**颜色构成的范围区域**，色域空间越大，所能表现的颜色越多。

HDR：High Danamic Range，高动态范围，比普通图像提供更多动态范围和图像细节，能够更好反应真实环境的视觉效果。颜色值经过归一化后，范围一般是[0,1]。而HDR可以表达超出1的颜色值，拥有更大的颜色范围。



### **旋转角度**

旋转角度：视频的YUV储存方向。一般的视频旋转角度是0°，对应的是横屏显示。后置摄像头竖屏拍的视频，旋转角度为90°，对应的是竖屏显示。Android中可以通过*MediaMetaDataRetriever*获取旋转角度。





### YUV

YUV是一种颜色编码系统，其中“Y”代表亮度（Luma），而“U”和“V”代表色度（Chroma）。YUV通常用于**数字视频和电视系统**中，其目的是将图像分成亮度和色度两个分量，从而在**保持图像质量的同时，减小图像数据的大小，更容易地传输和存储图像。**

其中，**亮度指的是图像的明暗程度，也就是灰度值**；而**色度指的是图像中颜色的饱和度和色调**。在YUV编码中，亮度和色度分量都是由原始RGB图像通过线性变换得到的。其中，Y分量代表了图像的亮度信息，而U和V分量则代表了色度信息。

在YUV编码中，Y分量占据了图像数据的大部分，而U和V分量则只占据了极少的空间。这样一来，通过**将图像数据压缩为YUV格式，可以极大地减小数据量**，从而更容易地传输和存储图像。同时，YUV编码还能够对图像数据进行压缩和解压缩，这在数字视频和电视系统中尤为重要。

总之，YUV编码系统通过将图像分成亮度和色度两个分量，可以在保持图像质量的同时，减小图像数据的大小，从而更容易地传输和存储图像。

> YUV是一种表示彩色视频或图像的颜色空间，其中Y表示亮度（luma），U和V分别表示色度（chroma）。
>
> - Y：表示亮度分量，也称为亮度信息，表示图像的明亮度或灰度值，决定了图像的明暗程度。在YUV颜色空间中，Y分量与RGB颜色空间中的R、G、B分量之间有如下的线性关系：Y = 0.299R + 0.587G + 0.114B。
> - U：表示蓝色色度分量，也称为色度差（chroma difference）分量，用于表示图像中蓝色与亮度之间的偏差。在YUV颜色空间中，U分量与RGB颜色空间中的R、G、B分量之间有如下的线性关系：U = -0.14713R - 0.28886G + 0.436B。
> - V：表示红色色度分量，也称为色度差分量，用于表示图像中红色与亮度之间的偏差。在YUV颜色空间中，V分量与RGB颜色空间中的R、G、B分量之间有如下的线性关系：V = 0.615R - 0.51498G - 0.10001B。
>
> YUV常用于视频编码和传输，因为人眼对亮度的敏感度比对色度的敏感度更高，所以可以通过对色度分量进行降采样来减小视频数据的大小，从而节省带宽和存储空间。



#### **YUV的数据格式是如何呢？**

YUV有两种分类方式，即“空间-间”和“空间-内”。“空间-间”的划分方式主要体现在Y、U、V的比例不同；“空间-内”的划分方式主要体现在Y、U、V的比例一定，存储格式不同。



#### **YUV“空间-间”的数据划分**

YUV按照“空间-间”的划分方式，分为YUV444、YUV422、YUV420，如下所示，假设图像为1920*1080：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/YUV1.png)



#### **YUV“空间-内”的数据划分**

YUV按照“空间-内”的划分方式，主要分为packet、planar、semi-planar三种：

◆ packet：打包格式，即先存储一个yuv，再存储下一个yuv；

◆ planar：平面格式，即先存储y平面，再存储u平面，再存储v平面；

◆ semi-planar：先存储y平面，再存储uv平面；



◆ YUV422各种存储格式如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/YUV2.png)

◆ YUV420各种存储格式如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/YUV3.png)

针对上图中的NV12、NV21、NV16、NV61说明：

◆NV：NV系列都属于semi-plane系列，“12”、“16”代表先U后V，“21”、“61”代表先V后U

◆ 12、16：代表一个像素占的位数



**4.YUV和RGB**

RGB：即red，green，blue三色存储空间，因音视频主要用的是YUV的色彩空间，感兴趣的小伙伴可以拓展下RGB相关知识，本文不再详述。介绍下RGB和YUV的转换公式：

◆ RGB 转 YUV：

  Y = 0.299R + 0.587G + 0.114B

  U= -0.147R - 0.289G + 0.436B

  V = 0.615R - 0.515G - 0.100B

◆ YUV 转 RGB：

  R = Y + 1.14V

  G = Y - 0.39U - 0.58V

  B = Y + 2.03U





### **时长**

视频所有图像播放所需要的时间称为视频时长。计算公式：**时长(s)=帧数x每帧时长=帧数x(1/帧率)**。假设一个视频帧数为1000，帧率为25fps，那么时长为40s。



### 视频编码格式/协议

视频经过解封装得到的视频轨数据，是经过编码的，所以**显示视频帧前需要解码**。不同编码算法组成不同编码协议，常见的有：H264(AVC，一般使用x264编码)、H265(HEVC，一般使用x265编码)、VP8、VP9、MPEG4、MJPEG、WMV3等。

#### 注！

**视频封装格式一般不包含视频的编码格式**，封装格式主要是用来组织和存储视频和音频数据，以及描述这些数据的元数据信息，如视频分辨率、帧率、音频采样率、编码格式、时间戳等等。

编码格式则是指将视频和音频等原始数据进行压缩编码的格式，如H.264、H.265、MPEG-2、MPEG-4、VP9等等。编码格式的作用是将原始数据压缩成较小的体积，以便于存储和传输。在视频封装格式中，一般会使用压缩后的视频和音频数据，而不是原始的未经过压缩编码的数据。因此，**视频封装格式和视频编码格式是两个不同的概念**，但**它们经常一起使用，以实现视频的存储和传输**。



#### H.264

##### **什么是H.264**

H.264是由ITU-T视频编码专家组（VCEG）和ISO/IEC动态图像专家组（MPEG）联合组成的联合视频组（JVT，Joint Video Team）提出的高度压缩数字视频编解码器标准。



##### 特点

1. **高压缩效率：** H.264采用了先进的压缩技术，包括运动补偿、帧内预测、变换和量化等，以提高视频压缩效率。这使得相对较小的比特率就能够保持相对高的视频质量。
2. **支持多种分辨率和帧率：** H.264支持多种分辨率和帧率设置，适用于不同的应用场景，从低分辨率的移动设备视频到高分辨率的高清电视。
3. **网络友好：** H.264在视频传输中具有较好的网络适应性。它能够在不同的网络带宽条件下提供稳定的视频传输，适应网络波动和丢包等情况。
4. **广泛应用：** H.264已经成为许多应用领域的标准，包括视频会议、实时视频传输、数字电视广播、互联网视频流媒体等。许多设备和平台都支持H.264编解码，使其成为一种通用的视频编码标准。
5. **提供多个配置和级别：** H.264标准定义了多个配置和级别，以满足不同应用的需求。这些配置和级别包括基本编码配置、主要编码配置、高级编码配置等。



##### **H.264的数据格式是怎样的**

H.264由视频编码层（VCL）和网络适配层（NAL）组成。

◆ VCL：H264编码/压缩的核心，主要负责将视频数据编码/压缩，再切分。

◆ NALU = NALU header + NALU payload



##### **VCL是如何管理H264视频数据**

◆ 压缩：预测（帧内预测和帧间预测）-> DCT变化和量化 -> 比特流编码；

◆ 切分数据，主要为了第三步。"切片(slice)"、“宏块(macroblock)"是在VCL中的概念，一方面提高编码效率和降低误码率、另一方面提高网络传输的灵活性。

◆ 包装成『NAL』。

◆ 『**VCL**』最后会被包装成『**NAL**』



##### **NAL头的数据结构体**

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/h.264-1.png)

◆ F（forbidden_zero_bit）：1 位，初始为0。当网络识别此单元存在比特错误时，可将其设为 1，以便接收方丢掉该单元

◆ NRI（nal_ref_idc）：2 位，用来指示该NALU 的重要性等级。值越大，表示当前NALU越重要。具体大于0 时取何值，没有明确规定

◆ Type（nal_unit_type）：5 位，指出NALU 的类型，如下所示：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/h.264-2.png)
1. **SPS（Sequence Parameter Set）**：SPS包含了视频序列的参数信息，例如视频的分辨率、帧率、编码方式等。**一个视频序列通常只有一个SPS，它描述了整个视频序列的基本特征**。SPS是一种全局参数，会被视频流中的所有帧使用。
2. **PPS（Picture Parameter Set）**：PPS包含了一组图像的编码参数信息，例如帧类型、帧间预测方式、QP值等。一个视频序列可以包含多个PPS，每个PPS对应一帧图像的编码参数。PPS是一种局部参数，仅对应单个图像的编码过程。


**NALU结构**
![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/NALU.png)

##### **H.264码流结构**

◆ H.264 = start_code + NALU（start_code：00000001 or 000001）

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/h264-3.png)



◆ 每个NAL前有一个起始码 0x00 00 01（或者0x00 00 00 01），解码器检测每个起始码，作为一个NAL的起始标识，当检测到下一个起始码时，当前NAL结束。

◆ 同时H.264规定，当检测到0x000000时，也可以表征当前NAL的结束。那么NAL中数据出现0x000001或0x000000时怎么办？H.264引入了防止竞争机制，如果编码器检测到NAL数据存在0x000001或0x000000时，编码器会在最后个字节前插入一个新的字节0x03，这样：

0x000000－>0x00000300

0x000001－>0x00000301

0x000002－>0x00000302

0x000003－>0x00000303



##### **I帧、P帧和B帧**

提到H.264，不得不提I帧、P帧、B帧、IDR帧、GOP。

◆ I帧（Intra-coded picture，帧内编码图像帧），表示关键帧，采用类似JPEG压缩的DCT(Discrete Cosine Transform，离散余弦变换)压缩技术，可达1/6压缩比而无明显压缩痕迹；

◆ P帧（Predictive-coded picture，前向预测编码图像帧），表示的是跟之前的一个关键帧或P帧的差别，P帧是参考帧，它可能造成解码错误的扩散；

◆ B帧（Bidirectionally predicted picture，双向预测编码图像帧），本帧与前后帧（I或P帧）的差别，B帧压缩率高，但解码耗费CPU；

◆ IDR帧（Instantaneous Decoding Refresh，即时解码刷新）：首个I帧，是立刻刷新,使错误不致传播，IDR导致DPB（DecodedPictureBuffer参考帧列表——这是关键所在）清空；在IDR帧之后的所有帧都不能引用任何IDR帧之前的帧的内容；IDR具有随机访问的能力，播放器可以从一个IDR帧播放。

◆ GOP（Group Of Picture，图像序列）：两个I帧之间是一个图像序列，一个GOP包含一个I帧



##### **解码时间戳和显示时间戳**

当然，H.264中还有两个重要的概念：DTS和PTS

◆ DTS（Decoding Time Stamp，解码时间戳解）：读入内存中的比特流在什么时候开始送入解码器中进行解码

◆ PTS（Presentation Time Stamp，显示时间戳）：解码后的视频帧什么时候被显示出来

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/h264-4.png)



## 音频



### 采样率

对声音信号**每秒的采样次数**，**采样率越高，声音的还原越真实**。采样率单位为Hz，常见的采样率有：8000Hz、16000Hz、44100Hz、48000Hz。人类一般能够听到的声音范围：20Hz～20000Hz。根据奈奎斯特采样定理：**当采样频率大于信号中最高频率的2倍时，采样后的数字信号能够完整保留原始信号的信息。**






### **声道**

指声音在录制或播放时，**在不同空间位置采集或回放的相互独立音频信号**。声道数指在录音时的**音源数量**，或者在播放时的扬声器数量。





### 声道布局

不同声道数对应不同声道布局。常见的声道布局有**单声道(mono)、立体声道(stereo)、四声环绕、5.1声道(杜比音效)**。

单声道：只有一个声道，优点数据量小，amr_nb和amr_wb默认为单声道，缺点是<u>缺乏对声音位置定位</u>。

立体声道：一般为两个声道，由<u>左声道、右声道组成，改善对声音位置定位的状况</u>。

四声环绕：由<u>前左、前右、后左、后右</u>组成，形成立体环绕。4.1声道是在四声环绕基础上，<u>增加一个低音</u>。

5.1声道：<u>在4.1基础上，增加一个中场</u>声道，杜比AC3就是采用5.1声道，也**就是影院宣传的杜比音效**。

### 音质

音质：声音的质量，经过编码压缩后的**音频信号保真度**，**由音量、音高和音色组成**。

音量：音频的强度，数值范围0-100，静音时为0，最大值为100。Android中有提供音量增强LoudnessEnhancer，调节声音分贝值。

音高：声音的音调，即音频频率或每秒变化次数。

音色：音频泛音，又称为音品，不同声音表现在波形方面与众不同的特性。





### 音频封装格式

音频的封装格式，与视频封装格式类似，由**特定格式头+媒体信息+音频轨数据**组成。常见的封装格式有：mp3、m4a、ogg、amr、wma、wav、flac、aac、ape等。





### 音频编码格式/协议

音频经过解封装得到的音频轨数据，也是经过编码的。常见的音频编码协议有：mp3、aac、amr_nb、amr_wb、ac3、vorbis、opus、flac、wmav2等。

#### 注

需要说明的还是音频封装格式和编码格式仍然不是同一个东西，而常说的MP3格式以及AAC、FLAC格式，它们既是封装格式也是编码格式，以MP3为例：

MP3既是一种音频编码格式，也是一种音频封装格式。MP3编码格式是一种有损压缩技术，能够将音频数据压缩至较小的文件大小，同时尽可能保持高质量的音频效果。而MP3封装格式则是将MP3编码产生的音频数据进行封装，使其成为一个完整的音频文件，其中包含了音频数据本身、文件头信息以及其他的元数据信息，方便播放器软件读取和解码。因此，MP3是同时具备编码和封装功能的一种音频格式。





### 采样数

采样数，即每帧采样的数量。在**FFmpeg的AVFrame中，定义为nb_samples**。





### 采样位数

采样位数，即**每个采样占用多少位**。在RIFF(Resource Interchange File Format)资源交换文件格式中字段bits_per_sample表示采样位数，在FFmpeg也是用这个字段表示采样位数。

音频采样位数是指在数字音频中用来表示**一个样本的比特数**。它决定了**每个样本的精度和范围**，通常用8位、16位、24位或32位来表示。比特数越大，表示每个采样的精度越高，音频信号的细节也越丰富，但同时也会占用更多的存储空间和带宽。一般来说，16位是目前最为常用的采样位数，可以提供足够的精度和动态范围，同时也具有较高的兼容性和广泛支持。但对于专业音频工程师或高要求的音频应用，可能会使用更高的采样位数以获得更高的音频质量和精度。





### 存储空间

音频的每秒存储空间由：采样率、声道数、每个采样位数。假设采样率为44.1k，声道数为2，采样位数为16。那么，每秒所占存储空间字节数=44100 * 2 * 16 / 8





### 码率

音频中的码率是指一个数据流中每秒能通过的信息量，单位为 b/s ，可以用以下公式计算：

码率 = 采样率 * 采用位数 * 声道数





### 帧时长

音频的帧时长=采样数 / 采样率。假设采样率为44.1k，采样数为1024。那么每帧时长约等于23ms。

音频帧时长也可以通过下面的公式计算：

帧时长 = 帧大小 ÷ 采样率 ÷ 采样位数 ÷ 声道数

其中，帧大小指的是一个音频帧包含的字节数，采样率是指每秒采样次数，采样位数是指每个采样值用多少位二进制表示，声道数是指音频信号的声道数。

例如，一个采样率为44100Hz、采样位数为16位、立体声的音频文件，每个音频帧的大小为4字节，那么该音频帧的时长为：

帧时长 = 4 ÷ 44100 ÷ 16 ÷ 2 = 0.00000226757 秒 (约为2.27毫秒)



### 音频采样格式

音频采样格式指的是在进行模拟信号数字化的过程中，将模拟信号采样后，转换为数字信号所采用的编码格式。

音频的采样格式分大端（big-endian）和小端（little-endian）两种。在大端格式中，高位字节存储在低地址，低位字节存储在高地址；而在小端格式中，低位字节存储在低地址，高位字节存储在高地址。按照符号划分有：有符号与无符号。按照类型划分有：整型与浮点型。按照存储位数划分有：8位、16位、32位、64位，都是8的倍数。

常见的音频采样格式包括有：

1. PCM（Pulse Code Modulation）脉冲编码调制格式：PCM是一种最基本的数字音频编码格式，将模拟音频信号按一定的采样率和量化位数转换成数字信号，可以被大多数音频处理软件所支持。
2. AAC（Advanced Audio Coding）高级音频编码格式：AAC是一种常见的有损压缩音频格式，广泛应用于音乐、电影、电视节目等领域。与MP3相比，AAC编码效率更高，音质更好。
3. MP3（MPEG-1 Audio Layer III）音频压缩格式：MP3是一种有损压缩的音频格式，可以将原始的音频信号压缩至原来的1/10或更小的体积。由于其高压缩率和较好的音质，在网络传输和存储方面得到了广泛的应用。
4. FLAC（Free Lossless Audio Codec）无损压缩格式：FLAC是一种无损压缩的音频格式，与MP3等有损压缩格式相比，它可以保留原始音频信号的所有信息，音质更好，但文件体积也较大。
5. WMA（Windows Media Audio）Windows媒体音频格式：WMA是微软公司开发的一种音频格式，与MP3等格式相比，它可以提供更高的音质和更小的文件体积，因此被广泛应用于网络传输和存储中。



#### PCM

##### **如何理解PCM**

PCM是一种用数字表示采样模拟信号方法。主要包括采样，量化，编码三个主要过程。

 ◆ 先来看看模拟信号采样的过程：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/PCM1.png)

 ◆ 采样率：每秒钟采样的样本数。比如我们常说的44.1kHz，即每秒钟采样44100次。

 ◆ 量化：将采样信号数据四舍五入到一个可用整数表示的过程。（位深）

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/PCM2.png)

 ◆ 编码：将量化后的信号转换成二进制数据。



##### **描述PCM数据的6个参数**

 ◆ Sample Rate : 采样频率。8kHz(电话)、44.1kHz(CD)、48kHz(DVD)。

 ◆ Sample Size : 量化位数。常见值为8-bit、16-bit。

 ◆ Number of Channels : 通道个数。常见的音频有立体声(stereo)和单声道(mono)两种类型，立体声包含左声道和右声道。另外还有环绕立体声等其它不太常用的类型。

 ◆ Sign : 表示样本数据是否是有符号位，比如用一字节表示的样本数据，有符号的话表示范围为-128 ~ 127，无符号是0 ~ 255。

 ◆ Byte Ordering : 字节序。字节序是little-endian还是big-endian。通常均为little-endian。

 ◆ Integer Or Floating Point : 整形或浮点型。大多数格式的PCM样本数据使用整形表示，而在一些对精度要求高的应用方面，使用浮点类型表示PCM样本数据。



#### AAC

##### **什么是AAC**

AAC(Advanced Audio Coding，高级音频编码)是一种声音数据的文件压缩格式。AAC分为ADIF和ADTS两种文件格式。



##### **什么是ADIF和ADTS**

 ◆ ADIF：Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不需进行在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。故这种格式常用在磁盘文件中（也即需要完整文件）。

 ◆ ADTS：Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始（也即更符合流式传输要求）。



##### **ADTS的数据结构**

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/AAC.png)

一般的基本结构：

1. **同步字（Syncword）:** 12 比特，固定为 `0xFFF`。标识一个ADTS帧的开始。
2. **MPEG版本（MPEG Version）:** 1 比特。用于指示MPEG版本，AAC一般为 0。
3. **Layer:** 2 比特。AAC使用的层级为 00。
4. **保护位（Protection Absent）:** 1 比特。指示是否有误码保护。0 表示有 CRC 校验，1 表示没有 CRC 校验。
5. **码率指示（Profile）:** 2 比特。表示AAC文件使用的编码配置。
6. **采样频率指示（Sampling Frequency Index）:** 4 比特。指示采样频率的索引值，对应于特定的采样频率。
7. **私有位（Private Bit）:** 1 比特。可以由编码器自行使用。
8. **声道配置（Channel Configuration）:** 3 比特。表示声道布局，例如立体声、单声道等。
9. **原始帧（Original Copy）:** 1 比特。指示是否为原始帧。
10. **主数据开始标志（Home）:** 1 比特。表示主数据开始的位置。
11. **帧长度（Frame Length）:** 13 比特。表示ADTS帧长度，包括ADTS头和AAC帧数据的长度。
12. **缓冲满度（Buffer Fullness）:** 11 比特。指示解码器缓冲的满度，用于解码同步。
13. **ADTS帧类型（ADTS Frame Type）:** 2 比特。指示ADTS帧的类型，如AAC LC（Low Complexity）或其他类型。
14. **CRC 校验（CRC Checksum）:** 16 比特。仅当“Protection Absent”为 0 时存在，用于检测帧数据是否受损。
> 关于AAC更详细内容可以看：[C++ 解析aac-adts的头部信息](https://blog.csdn.net/u013113678/article/details/123134860)





## 字幕

### 定义

视频字幕是一种以文本形式显示在视频屏幕上的文字信息，其目的是提供对话、解释性信息或其他内容，以帮助观众理解视频内容。字幕可以在视频的底部或其他位置显示，通常与说话者的语音同步。视频字幕广泛应用于电影、电视节目、在线视频和其他多媒体制品中。



### 常见分类

#### 按文件格式来区分

1. **ASS（Advanced SubStation Alpha）和 SSA（SubStation Alpha）：**

   - ASS 和 SSA 是一种高级字幕格式，用于存储关于字幕文本、样式、效果和显示时间的详细信息。

   - ASS 是 SSA 的进阶版本，提供了更多的功能和控制选项。

   - 这两种格式通常用于动画和电影制作，支持更丰富的文本样式和动画效果。

   - **SSA/ASS的基本结构：**

     SSA/ASS字幕是一种类ini风格纯文本文件；包含五个section：[Script Info]、[v4+ Styles]、[Events]、[Fonts]、[Graphics]。

     ◆ [Script Info]：包含了脚本的头部和总体信息。[Script Info] 必须是 v4 版本脚本的第一行。

     ◆ [v4 Styles]：包含了所有样式的定义。每一个被脚本使用的样式都应该在这里定义。ASS 使用 [v4+ Styles]。

     ◆ [Events]：包含了所有脚本的事件，有字幕、注释、图片、声音、影像和命令。基本上，所有在屏幕上看到的内容都在这一部分。

     ◆ [Fonts]：包含了脚本中内嵌字体的信息。

     ◆ [Graphics]：包含了脚本中内嵌图片的信息。

     SSA字幕样本范例如下：
     ![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/SSA.png)

2. **SRT（SubRip）：**

   - SRT 是一种简单的字幕文件格式，无样式和效果，其文件格式非常简洁，每个字幕事件由序号、显示时间和对话文本组成，只包含了文本和显示时间信息，是一种常见的、简单易读的字幕格式。
   - SRT 文件是纯文本文件，通常使用 UTF-8 或 ASCII 编码。这意味着可以使用文本编辑器轻松创建和编辑 SRT 文件。

3. **XML（eXtensible Markup Language）：**

   - XML 是一种通用的标记语言，也可以用于表示字幕信息。
   - XML 字幕格式通常更灵活，允许存储更多的元数据和样式信息。

4. **TXT（Plain Text）：**

   - TXT 字幕文件是纯文本文件，其中包含简单的文本信息，通常不包括样式和其他高级特性。
   - TXT 字幕文件是最简单和最基本的字幕格式，通常用于较为简单的字幕需求。

#### 按字幕文件与视频文件关联方式区分

1. **内嵌字幕（Hard Subtitles）：**

   - **定义：** 内嵌字幕是直接嵌入到视频图像中的字幕，成为视频的一部分。
   - **特点：** 这意味着字幕无法通过调整设置或关闭字幕来取消显示，因为它们被直接嵌入到视频中。
   - **优点：** 内嵌字幕对于无法编辑视频的平台和设备来说是一种方便的方式，确保字幕始终与视频一起播放。

2. **外挂字幕（Soft Subtitles）：**

   - **定义：** 外挂字幕是作为额外的字幕轨道存储在视频文件中，而不是嵌入到视频图像中。
   - **特点：** 观众可以选择是否显示或隐藏外挂字幕，通常在播放器的设置中进行选择。
   - **优点：** 外挂字幕允许用户选择显示的字幕语言、样式和其他设置，使其更加灵活。
   - **注：**对于外挂字幕，播放器需要支持相应的字幕格式，以正确解析和显示字幕。大多数播放器支持常见的字幕格式，如ASS、SSA、SRT 等。

   



## 音视频编码

### 缘由

为什么需要编码呢？

我们举一个例子：
以一个分辨率为` 1920 * 1080 `像素且帧率为` 30 f/s` 的视频为例，共有 1920 * 1080 = 2073600 像素， 每像素 24b（假设采取RGB24），也就是单帧图片 2073600 * 24b = 49766400b = 6220800B ≈ 6.22MB，那么每秒视频大小 6.22 MB * 30 = 186.6MB，那么一分钟就是11GB，一部`90分钟`的电影，约为 `990GB`

而显然，对于视频，由于画面是逐渐过渡的，因此在整个视频中，包含大量画面/像素的重复，这会**包含大量0和1的重复数据，这正好提供了非常大的压缩空间。**



### 视频编码

主流视频编码有 ITU（International Telecommunication Union，国际电信联盟）的H.26x（1/2/3/4/5）系列、MPEG（Motion Picture Experts Group, 运动图像专家组）的MPEG（1/2/3/4）系列



#### 编码举例

H.264 : H.264 / MPEG-4 第十部分，或称为 AVC（Advanced Video Coding，高级视频编码），是一种视频压缩标准，也是一种被广泛使用的高精度视频的录制、压缩和发布格式。

H.265：是H.264 / MPEG-4 AVC的继承者，HEVC（High Efficiency Video Coding， 高效率视频编码），是一种视频压缩标准，被认为不仅提升了图像质量，同时也能达到H.264 / MPEG-4 AVC两倍的压缩率（等同于同样画面质量下比特率减少了50%），可以支持4K分辨率甚至超高画质电视，最高分辨率可达 8129 * 4320（8K分辨率），它是目前发展的趋势。





### 音频编码

#### 原始数据

音频数据的承载方式最常用的是PCM（**P**ulse-**c**ode **m**odulation，脉冲编码调制）是一种模拟信号的数字化方法。PCM将信号的强度依照同样的间距分成数段，然后用独特的数字记号（通常是二进制）来量化。PCM常用于数字电信系统上，也是电脑和红皮书中的标准形式。



#### 缘由

原始的PCM音频数据包含非常大的数据量(PCM并不流行于诸如DVD或DVR的消费性商品上，因为它需要相当大的比特率（DVD格式虽然支持PCM，不过很少使用）)，因此音频也有了很多种压缩方式WAV、MP3、WMA、APE、FLAC（后两种为音频无损压缩技术）





## 音视频播放原理

### 音视频播放分类

音视频播放可以分为两种情况：在线播放和本地播放。

1. **在线播放：**
   - **定义：** 在线播放是指通过互联网直接从服务器上获取音视频内容，并实时流式传输到用户的终端设备，用户在播放过程中不需要下载完整的文件。
   - **特点：**
     - **即时性：** 用户可以即时访问和播放媒体内容，无需等待完整下载。
     - **节省存储空间：** 无需在本地设备上存储完整的音视频文件，节省了存储空间。
     - **多样化内容：** 可以随时获取各种不同类型的音视频内容。

2. **本地播放：**
   - **定义：** 本地播放是指用户在本地设备上拥有完整的音视频文件，通过本地储存介质（如硬盘、SD卡等）进行播放，无需网络连接。
   - **特点：**
     - **独立性：** 用户不依赖互联网连接，可以在没有网络的情况下随时播放。
     - **高清晰度：** 由于文件完整存在于本地设备上，播放过程中不受网络速度限制，有助于实现高清晰度播放。
     - **自主管理：** 用户可以自由管理和组织本地存储的音视频文件，不受在线服务器存储空间的限制。

总的来说，在线播放适用于需要即时访问、不占用本地存储空间的场景，而本地播放适用于用户希望在没有网络连接或对高清晰度播放有要求的情况。在实际应用中，很多音视频播放平台同时支持在线和本地播放，以满足用户在不同场景下的需求。





## 音视频封装格式

首选需要弄清楚的概念是：视频封装格式是**将视频编码数据和音频编码数据以及其他元数据<u>进行封装的文件格式</u>**。视频格式通常由一个容器格式和一个或多个音视频编码格式组成。**容器格式(即封装格式)负责存储和传输数据**，而**音视频编码格式则负责对音频和视频数据进行压缩和编码**。

视频的封装格式，由特定格式头+媒体信息+音视频轨(字幕)数据+视频轨索引组成。常见的封装格式有：mp4、mkv、webm、avi、3gp、mov、wmv、flv、mpeg、asf、rmvb等。



> 常见的视频封装格式包括：
>
> 1. AVI(Audio Video Interleave)：是由微软公司开发的视频封装格式，最早出现在Windows 3.1操作系统中，支持多种视频和音频编码格式。由于不支持流媒体传输，目前使用较少。
> 2. MP4(MPEG-4 Part 14)：是一种常用的视频封装格式，可支持多种音视频编码格式，例如H.264、AAC等，广泛用于互联网视频流媒体传输。
> 3. MKV(Matroska)：是一种开源的视频封装格式，可以容纳多种不同编码方式的音频、视频和字幕轨道。MKV支持高清视频和多语言音轨等特性，也被广泛用于互联网视频流媒体传输。
> 4. MOV(QuickTime File Format)：是由苹果公司开发的视频封装格式，支持多种视频和音频编码格式，也支持流媒体传输。常用于苹果设备和软件中。
> 5. FLV(Flash Video)：是一种Adobe公司开发的视频封装格式，适用于Adobe Flash Player播放器，支持流媒体传输，常用于网络视频传输。



### FLV

FLV（Flash Video），Adobe公司设计开发的一种流行的流媒体格式，由于其视频文件体积轻巧、封装简单等特点，使其很适合在互联网上进行应用。除了播放视频，在直播时也可以使用。采用FLV格式封装的文件后缀为.flv，格式如下（FLV = FLV Header + Body）：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV1.png)



#### **FLV Header**

FLV头通常由9个字节组成，其中包括3个字节的“FLV”签名，表示文件类型，后跟一个版本号，保留位和标志位。FLV头中的一些字段包含了一些关于文件的基本信息。大于9个字节则表示头部信息在这基础之上还存在扩展数据。FLV Header 的信息排布如下所示：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV2.png)



#### **FLV Body**

FLV体是整个FLV文件的主体，包含一个或多个FLV标签。每个FLV标签都包含了音频、视频或元数据的一部分。FLV体的结构是按照时间戳进行排序的，用于正确的时间基础流。换句话来说，Body 是由一个个Tag组成的，每个Tag下面有一块4个字节的空间，用来记录这个Tag 的长度。这个后置的PreviousTagSize用于逆向读取处理，表示的是前面的Tag的大小。FLV Body 的信息排布如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV3.png)

> 关于逆向读取：`PreviousTagSize` 是 FLV 文件中的一个关键元素，它存储了前一个 FLV 标签的大小。逆向读取是指从文件的末尾向前读取，以获取文件中的信息。
>
> 在 FLV 文件中，每个 FLV 标签都以 `PreviousTagSize` 字段结束，该字段存储了前一个标签的大小（以字节为单位）。这样，可以从文件的末尾开始向前逐个读取每个标签，并通过读取 `PreviousTagSize` 字段来找到前一个标签的边界。
>
> 逆向读取 `PreviousTagSize` 的基本步骤如下：
>
> 1. 打开 FLV 文件，并定位到文件的末尾。
> 2. 读取最后一个标签的 `PreviousTagSize` 字段。这将告诉你前一个标签的大小。
> 3. 根据前一个标签的大小，再次向前移动，并读取前一个标签的 `PreviousTagSize` 字段。
> 4. 重复这个过程，直到到达文件的开头。
>
> 为什么要进行逆向读取呢？因为在 FLV 文件中，标签的顺序是按照时间戳排序的，而逆向读取 `PreviousTagSize` 字段可以帮助你在文件中找到每个标签的位置，而不必从头到尾读取整个文件。这对于**快速定位和检索特定时间范围内的标签信息**是非常有用的，特别是对于较大的 FLV 文件而言。



#### **FLV Tag**

FLV标签是FLV文件的基本单元，用于包装音频、视频或元数据。每个FLV标签包含了一个标签头和标签数据。

1. **标签头（Tag Header）:** 包含了标签的类型（音频、视频或脚本数据）和标签数据的数据长度、时间戳、时间戳扩展、StreamsID等信息。
2. **标签数据（Tag Data）:** 包含了实际的音频、视频帧或元数据。

Tag结构如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV4.png)

TAG Header 通常由 11 字节组成，其中包括：

1. **Tag Type（1 字节）：**
   - 标识 TAG 的类型，可以是音频（8）、视频（9）、或者脚本数据（18）。
2. **Data Size（3 字节）：**
   - 表示 TAG Data 部分的大小（以字节为单位）。
3. **Timestamp（4 字节）：**
   - 表示 TAG 的时间戳。对于音频和视频 TAG，这是相对时间戳；对于脚本数据 TAG，这是绝对时间戳。
4. **Timestamp Extended（1 字节）：**
   - 表示 Timestamp 的扩展，用于表示更长的时间戳。
5. **Stream ID（3 字节）：**
   - 表示 TAG 所属的流的 ID。通常为 0。



#### **Tag Data**

TAG Data 部分包含了实际的音频、视频帧或脚本数据，其结构和内容取决于 TAG 的类型。

1. **音频 TAG Data：**
   - 音频 TAG 的 TAG Data 结构包括音频格式信息和音频帧数据。
2. **视频 TAG Data：**
   - 视频 TAG 的 TAG Data 结构包括视频编码格式信息和视频帧数据。
3. **脚本数据 TAG Data：**
   - 脚本数据 TAG 的 TAG Data 结构包含 ActionScript 数据等脚本信息。



##### **Audio Tag Data**

音频的Tag Data又分为 AudioTagHeader 和 Data 数据区，其排布结构如下图所示：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV5.png)

Audio TAG Header 通常包括以下字段：

1. **Sound Format（4 bits）：**
   - 定义音频数据的格式。可能的值包括 0（Linear PCM）、1（ADPCM）、2（MP3）、3（Linear PCM Little Endian）、6（Nellymoser 16 kHz mono）、7（Nellymoser 8 kHz mono）、10（AAC）、11（Speex）、14（MP3 8 kHz）、15（Device-specific sound）。
2. **Sound Rate（2 bits）：**
   - 定义音频数据的采样率。可能的值包括 0（5.5 kHz）、1（11 kHz）、2（22 kHz）、3（44 kHz）。
3. **Sound Size（1 bit）：**
   - 定义音频数据的采样大小。0 表示 8-bit，1 表示 16-bit。
4. **Sound Type（1 bit）：**
   - 定义音频数据的声道类型。0 表示单声道，1 表示立体声。
5. **AAC Packet Type（8 bits）：**
   - 对于 AAC 编码，这个字段定义 AAC 数据包的类型。

根据 Sound Format 的不同，Audio TAG Data 的结构也会有所不同。

1. **Linear PCM：**
   - 包含原始的 PCM 音频数据，可以是 8-bit 或 16-bit。
2. **ADPCM：**
   - 包含 ADPCM 编码的音频数据。
3. **MP3：**
   - 包含 MP3 编码的音频数据。
4. **AAC：**
   - 包含 AAC 编码的音频数据。此时，Sound Format 为 10，且 AAC Packet Type 字段指示 AAC 数据包类型。
5. **Nellymoser：**
   - 包含 Nellymoser 编码的音频数据。
6. **Speex：**
   - 包含 Speex 编码的音频数据。
7. **Device-specific Sound：**
   - 包含设备特定的音频数据。





##### **Video Tag Data**

Video Tag 由一个字节的VideoTagHeader 和 Video数据区部分组成

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV6.png)

Video数据区部分格式不确定。对于H.264/AVC编码部分，Video数据区排布如下:

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV7.png)

Video TAG Header 通常包括以下字段：

1. **Frame Type（4 bits）：**
   - 定义视频帧的类型。可能的值包括 1（keyframe）、2（inter frame）、3（disposable inter frame）。
2. **Codec ID（4 bits）：**
   - 定义视频编解码器的类型。可能的值包括 2（Sorenson H.263）、3（Screen video）、4（On2 VP6）、7（AVC）。

根据 Codec ID 的不同，Video TAG Data 的结构也会有所不同。

1. **Sorenson H.263：**
   - 包含 Sorenson H.263 编码的视频数据。
2. **Screen Video：**
   - 包含 Screen video 编码的视频数据。
3. **On2 VP6：**
   - 包含 On2 VP6 编码的视频数据。
4. **AVC（H.264）：**
   - 包含 AVC 编码（H.264）的视频数据。



##### **Script Tag Data**

FLV 文件中的 Script TAG Data 部分包含了脚本数据 TAG 的详细信息。Script TAG 主要用于包含 ActionScript 3.0 数据，这是一种基于 ECMAScript 的脚本语言，用于实现与 Flash 应用程序的交互。以下是 Script TAG Data 的一般结构：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/FLV8.png)

Script TAG Header 包含以下字段：

1. **Data Type（8 bits）：**
   - 定义 TAG 中的脚本数据类型。通常为 0x12（表示脚本数据类型为字符串）。
2. **Data Size（24 bits）：**
   - 定义 TAG 中脚本数据的大小（以字节为单位）。

Script TAG Data 部分包含脚本数据，通常是 ActionScript 3.0 代码。

在解析 Script TAG Data 时，需要注意：

1. **脚本数据类型：** 根据 Data Type 字段的值，你可以确定 Script TAG 中包含的脚本数据的类型。通常为字符串。
2. **脚本数据大小：** 根据 Data Size 字段的值，你可以确定脚本数据的大小，从而准确地读取脚本数据。



### TS

TS（Transport Stream，传输流），一种常见的视频封装格式，是基于MPEG-2的封装格式（所以也叫MPEG-TS），后缀为.ts。虽然在互联网视频传输中，基于 HTTP 的协议（如 HLS，DASH）更为流行，但是TS协议在特定领域，如数字广播、有线电视、卫星电视等传统的广播和传输领域，仍然是一个重要的封装格式和传输协议。以下是一些 TS 协议的应用场景：

1. **数字广播：** TS 在数字电视和广播领域仍然是主要的传输协议。数字广播系统通常使用 TS 封装来传输视频、音频、数据等多路信号。
2. **有线电视：** TS 被广泛用于有线电视系统，将多个节目流（视频、音频等）合并到一个传输流中。
3. **卫星电视：** 卫星电视广播系统也常使用 TS 来传输多个频道的信号。
4. **监控系统：** 在视频监控和安防领域，TS 也用于传输和存储监控摄像头的视频流。
5. **教育和培训：** 一些在线教育平台和培训系统可能使用 TS 来传输视频流，特别是在需要实时传输和同步的情境下。
6. **专业视频设备：** 在一些专业视频设备和广播设备中，TS 仍然是常见的封装格式。



#### **TS的分层结构**

TS文件分为三层，如下所示（可以倒序看更好理解）：

◆ TS层（Transport Stream）：在PES层基础上加入了数据流识别信息和传输信息。

◆ PES层（Packet Elemental Stream）：在ES层基础上加入时间戳（PTS/DTS）等信息。

◆ ES层（Elementary Stream）：压缩编码后的音视频数据。

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS1.png)



#### **TS层**

**◆ ts包大小固定为188字节**，ts层分为三个部分：ts header、adaptation field、payload。

◆ ts header固定4个字节；

◆ adaptation field可能存在也可能不存在，主要作用是给不足188字节的数据做填充。

◆ payload是 PES 数据，或者PAT，PMT等。

◆ ts Header + adaptation field 格式如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS2.png)



##### **TS Header**

◆ TS Header格式如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS3.png)

- **Sync Byte（同步字节）：** 始终为0x47，用于同步解析器。
- **Transport Error Indicator（传输错误指示器）：** 表示是否有传输错误，如果为1，表示存在错误。
- **Payload Unit Start Indicator（负载单元起始指示器）：** 标志着该包为PES包或者其他单元的起始，如果为1，表示该包的开始处有PES包。
- **transport priority（传输优先级）：**表示该TS包传输的优先级，1为高，0为低，默认0.
- **PID（包标识符）：** 13位PID用于唯一标识不同的数据流，例如视频、音频等。
- **Transport Scrambling Control（传输加扰控制）：** 指示是否对传输包进行加扰，通常为00表示不加扰。
- **Adaptation Field Control（自适应字段控制）：** 指示传输包中是否包含自适应字段（Adaptation Field）。
- **Continuity Counter（连续性计数器）：** 4位计数器，用于指示同一PID的传输包之间的连续性。



◆ 其中pid 决定了负载内容的类型，主要包括：PAT表，PMT表，视频流，音频流。常用的PID值：

| 表   | **PAT** | CAT    | TSDT   | EIT,ST | RST,ST | TDT,TOT,ST |
| ---- | ------- | ------ | ------ | ------ | ------ | ---------- |
| PID  | 0x0000  | 0x0001 | 0x0002 | 0x0012 | 0x0013 | 0x0014     |

1. **PAT（Program Association Table）：**
   - **PID：** 0x0000
   - **意义：** PAT 包含了一个用于映射Program Map Table (PMT)的表。PMT 包含了与每个程序相关的信息，例如音频、视频的PID等。
2. **CAT（Conditional Access Table）：**
   - **PID：** 0x0001
   - **意义：** CAT 包含有关条件访问系统的信息，例如加扰控制信息等。
3. **TSDT（Transport Stream Description Table）：**
   - **PID：** 0x0002
   - **意义：** TSDT 包含有关整个 Transport Stream 的描述信息。
4. **EIT（Event Information Table）：**
   - **PID：** 0x0012
   - **意义：** EIT 包含有关当前和未来事件（节目）的信息，例如节目的开始时间、结束时间、描述等。
5. **RST（Running Status Table）：**
   - **PID：** 0x0013
   - **意义：** RST 包含有关正在播放的节目的信息，如是否处于运行状态等。
6. **TDT（Time and Date Table）：**
   - **PID：** 0x0014
   - **意义：** TDT 包含了当前传输流的UTC时间和日期信息。TOT（Time Offset Table）也经常与TDT一起使用，提供了本地时间偏移的信息。



##### **调整字段**

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS4.png)



##### **PAT表结构（指明PMT表的PID值）**

```c
typedef struct TS_PAT{
    unsigned table_id                 :8  //固定位0x00，表示该表是PAT
    unsigned section_syntax_indicator :1  //段语法标志，固定为1
    unsigned zero                     :1  //固定为0
    unsigned reserved_1               :2  //第一个保留位
    unsigned section_length           :12 //表示这个字节之后有用的字节数，包括CRC_32
    unsigned transport_stream_id      :16 //传输流的ID，区别于一个网络中其他多路复用的流
    unsigned reserved_2               :2  //第二个保留位
    unsigned version_number           :5  //表示PAT的版本号
    unsigned current_next_indicator   :1  //表示发送的PAT是当前有效还是下一个有效，为1时代表当前有效
    unsigned section_number           :8  //如果PAT分段传输，那么此值每次递增1
    unsigned last_section_number      :8  //最后一个分段的号码
    for(int i=0;i<N;i++){
        unsigned Program_number           :16 //节目号
        unsigned Reversed_3               :3  //保留位
        if(Program_number == 0)
            Network_id                    :13 //网络信息表（NIT）的PID
        else
            Program_MAP_PID               :13 //节目映射表的PID，每个节目都有一个
    }
    unsigned CRC_32                   :32 //CRC32校检码
}TS_PAT
```





##### **PMT表（指明音视频流的PID值）**

```c
typedef struct TS_program_map_section{
    unsigned Table_id                    :8  //标志PSI分段的内容，对于PMT，此值为0x02
    unsigned Section_syntax_indicator    :1  //置为1
    unsigned '0'                         :1
    unsigned Reserved                    :2  //保留位
    unsigned Section_length              :12 //指明了自此到最后CRC_32的字节数
    unsigned Program_number              :16 //指出该节目的节目号，与PAT表对应
    unsigned Reserved                    :2  //保留位
    unsigned Version_number              :5  //取值0-31，代表当前PMT的版本号
    unsigned Current_next_indicator      :1  //代表当前PMT是否有效
    unsigned Section_number              :8  //给出了当前所处段的数目
    unsigned Last_section_number         :8  //给出了最后一个分段，即分段的最大数目
    unsigned Reserved                    :3  //保留位
    unsigned PCR_PID                     :13 //指示TS包的PCR值，该TS包含有PCR字段
    unsigned Reserved                    :4  //保留位
    unsigned Program_info_length         :12 //该字段描述跟随其后对节目信息描述的字节数
    for(int i = 0; i < N; i++)
        Descriptr()
    for(int i = 0; i < N; i++){
        unsigned Stream_type             :8  //0x00：保留， 0x01：MPEG1视频，0x02：MPEG2视频，0x03:MPEG1音频，0x04：MPEG2音频，0x05：私有字段，0x06：含有私有数据的PES包 ......
        unsigned Reserved                :3  //保留
        unsigned Elementary_PID          :13 //指示TS包的PID，这些TS包含有相同的PID
        unsigned Reserved                :4  //保留
        unsigned ES_info_length          :12 //指示跟随其后描述相关节目元素的字节数
        for(int j = 0; j < N2; j++)
            Descriptr()
    }
    unsigned CRC_32                      :32 //循环校检位
}
```

在 MPEG-2 Transport Stream（TS）中，一个 PID（Packet Identifier）的含义可以依赖于它所在的表的上下文。同一个 PID 在不同的表中可能有不同的用途和解释。

1. **PAT 表中的 PID：**
   - 在 PAT 表中，一个特殊的 PID 通常为0x0000。这个 PID 对应整个传输流中的 PAT 表。通过解析 PAT 表，可以找到每个节目对应的 Program Map Table（PMT）的 PID。
2. **PMT 表中的 PID：**
   - 在 PMT 表中，每个 Elementary Stream（音频、视频、字幕等）都有一个独特的 PID。这个 PID 用于标识传输流中特定类型的数据流。

同一个 PID 在不同的表中可能有不同的语境和用途，因此解析 TS 流时需要根据所在的表来正确解释 PID 的含义。这种设计允许 TS 流在传输多个节目的情况下有效地组织和描述信息。通过解析 PAT 表，接收端能够找到每个节目对应的 PMT 表的 PID，从而正确解释 TS 流中的音频、视频等多媒体内容。



##### **补充说明**

◆ 打包ts流时PAT和PMT表是没有调整字段的，不够的长度直接补0xff即可。

◆ 视频流和音频流都需要加adaptation field，通常加在一个帧的第一个ts包和最后一个ts包里，中间的ts包不加。





#### **PES 层**

◆ PES（Packetized Elementary Stream，打包的ES），在 ES 层的基础上加入了时间戳（PTS/TDS）等信息。

◆ ES数据包比较大，加入PES头时需将ES进行分割，只在第一个分割的ES上加PES头，如下图所示

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS9.png)

◆ PES packet length — 指示PES 包中跟随该字段最后字节的字节数。0 值指示PES 包长度既未指示也未限定并且仅在这样的PES 包中才被允许，该PES 包的有效载荷由来自传输流包中所包含的视频基本流的字节组成。

◆ PES结构如下：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS7.png)



◆ PES 关键字段说明

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/TS8.png)

- **Packet Start Code Prefix（包起始码前缀）：** 始终为0x000001，用于指示PES包的开始。
- **Stream ID（流标识符）：** 8位标识符，指示了数据流的类型，例如视频、音频等。
- **PES Packet Length（PES包长度）：** 16位字段，指示了PES包的长度（包括PES头和负载）。
- **PES Scrambling Control（PES加扰控制）：** 指示PES包是否被加扰。
- **PES Priority（PES优先级）：** 指示PES包的优先级。
- **Data Alignment Indicator（数据对齐指示器）：** 表示PES包中的数据是否按字节对齐。
- **PES Copy Control（PES复制控制）：** 指示是否允许对PES包中的数据进行拷贝。
- **PES Extension Flag（PES扩展标志）：** 标志是否有PES扩展字段。
- **PES CRC Flag（PES CRC标志）：** 指示是否有CRC校验。
- **PES Extension Flag 2（PES扩展标志2）：** 标志是否有PES扩展字段2。
- **PES Header Data Length（PES头数据长度）：** 指示PES头中附加数据的长度。
- **PES Header（PES头）：** 包含有关PES包的附加信息，例如时间戳等。
- **PES Payload（PES负载）：** 原始的音频、视频等媒体数据。



#### **ES 层**

◆ ES（Elementary Stream，基本码流），就是音视频编码数据流，比如视频H.264，音频AAC。

◆ 一个 ES 流中只包含一种类型的数据（视频，或音频，或字幕）。



### **MP4（MPEG-4）**

◆ MP4是一套用于音频、视频信息的压缩编码标准，由国际标准化组织（ISO）和国际电工委员会（IEC）下属的“动态图像专家组”（Moving Picture Experts Group，即MPEG）制定，第一版在1998年10月通过，第二版在1999年12月通过。MPEG-4格式的主要用途在于网上流、光盘、语音发送（视频电话），以及电视广播。

◆ MP4由许多box组成，每个box包含不同的信息，这些box以树形结构的方式组成，box 当中可以包含 box 如下所示：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/MP4.png)





#### box

mp4文件由若干个box组成。下面是box结构的一个示意图：

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/MP4-1.png)

◆ box 由 header 和 body 组成，header 指明 box 的 size 和 type。size 是包含 box header 的整个 box 的大小。

◆ box type，通常是4个ASCII码的字符如“ftyp”、“moov”等，这些 box type都是已经预定义好的，表示固定的含义。如果是“uuid”，表示该box为用户自定义扩展类型，如果 box type 是未定义的，应该将其忽略。

◆ 如果 header 中的 size 为1，则表示 box 长度需要更多的bits位来描述，在后面会有一个 8自己 位的 largesize 用来描述 box 的长度。如果 size 为0，表示该 box 为文件的最后一个box，文件结尾（同样只存在于“mdat”类型的box中）。

◆ box 分为两种，Box 和 Fullbox。FullBox 是 Box 的扩展，Header 中增加了version 和 flags字段。

◆ box 定义如下：

```c
aligned(8) class Box (unsigned int(32) boxtype,
    optional unsigned int(8)[16] extended_type) {
    unsigned int(32) size;
    unsigned int(32) type = boxtype;
    if (size==1) {
        unsigned int(64) largesize;
    } else if (size==0) {
    // box extends to end of file
    }
    if (boxtype==‘uuid’) {
        unsigned int(8)[16] usertype = extended_type;
    }
}
```

◆ FullBox 定义如下：

```c
aligned(8) class Box (unsigned int(32) boxtype,
    optional unsigned int(8)[16] extended_type) {
    unsigned int(32) size;
    unsigned int(32) type = boxtype;
    if (size==1) {
        unsigned int(64) largesize;
    } else if (size==0) {
    // box extends to end of file
    }
    if (boxtype==‘uuid’) {
        unsigned int(8)[16] usertype = extended_type;
    }
}
```



#### ISO/IEC 14496-12（ISOBMFF）

ISO/IEC 14496-12 是 MPEG-4 标准的一部分，具体而言是 MPEG-4 Part 12 标准，也称为 ISO Base Media File Format（ISOBMFF）。这个标准定义了一种通用的媒体文件格式，用于存储多媒体数据，包括音频、视频、字幕和其他相关的媒体信息。

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/MP4-2.png)

> ### Box Types:
>
> 1. **ftyp - File Type Box:**
>    - **Structure:**
>      - Major Brand (4 bytes)
>      - Minor Version (4 bytes)
>      - Compatible Brands (variable)
>
> 2. **moov - Movie Box:**
>    - **Structure:**
>      - Movie Header Box (mvhd)
>      - Track Box (trak) - can be multiple
>        - Track Header Box (tkhd)
>        - Media Box (mdia)
>          - Media Header Box (mdhd)
>          - Handler Reference Box (hdlr)
>          - Media Information Box (minf)
>            - Sample Table Box (stbl)
>              - Sample Description Box (stsd)
>              - Time to Sample Box (stts)
>              - Composition Time to Sample Box (ctts) (optional)
>              - Sample Size Box (stsz)
>              - Sample To Chunk Box (stsc)
>              - Chunk Offset Box (stco or co64)
>
> 3. **stbl - Sample Table Box:**
>    - **Structure:**
>      - Sample Description Box (stsd)
>      - Time to Sample Box (stts)
>      - Composition Time to Sample Box (ctts) (optional)
>      - Sample Size Box (stsz)
>      - Sample To Chunk Box (stsc)
>      - Chunk Offset Box (stco or co64)
>
> 4. **mdat - Media Data Box:**
>    - **Structure:**
>      - Box Size
>      - Box Type ('mdat')
>      - Media Data
>
> ### Structure and Cross-Reference:
>
> - **ftyp - File Type Box:**
>   - 主要品牌（Major Brand）指定了文件的类型，而兼容品牌（Compatible Brands）列举了文件兼容的其他品牌。
>
> - **moov - Movie Box:**
>   - `moov` 框包含整个电影的信息，其中 `mvhd` 框指定了整体时长和时间刻度。每个 `trak` 框对应一个时间轨道，其中包含 `tkhd` 框定义的轨道信息和 `mdia` 框包含的媒体信息。
>
> - **stbl - Sample Table Box:**
>   - `stbl` 框包含了有关媒体样本的详细信息。`stsd` 框定义了样本的描述，而 `stts` 和 `ctts` 框提供了样本时间和合成时间信息。`stsz` 框指定了样本的大小，而 `stsc` 和 `stco` 框定义了样本分块和偏移信息。
>
> - **mdat - Media Data Box:**
>   - `mdat` 框包含了实际的媒体数据，如音频和视频样本。该框的大小（Box Size）指定了实际媒体数据的大小。
>
> 这只是 ISOBMFF 结构的一小部分，实际上，还有更多类型的box用于支持其他功能，如片段（segments）、广告、文件索引等。 ISOBMFF 通过框之间的嵌套和引用关系提供了一种灵活的方式，以组织和描述多媒体文件。详细信息可以在 ISO/IEC 14496-12 标准中找到。





## 音视频播放原理

![](https://github.com/drizzledrop3/audio-video-learning/blob/main/IMG/AudioVideo/音视频播放原理图.png)

音视频播放的原理主要分为：解协议->解封装->解码->音视频同步->播放。当然，如果是本地播放，没有解协议这一步骤。

1. **解协议：**

   - **定义：** 在音视频播放开始之前，首先要进行解协议的过程。这一步骤涉及到从网络获取音视频数据流，通常使用的是协议，比如HTTP、RTSP等。解协议的任务是建立与服务器的连接，发起请求并接收响应，以获取音视频数据流的位置和相关信息。
   - **适用情境：** 主要在在线播放中发挥作用，因为本地播放不涉及网络传输。

2. **解封装：**

   - **定义：** 解封装是将从网络获取的音视频数据流按照特定的封装格式进行解析，提取出其中的音频和视频数据。常见的封装格式包括MP4、FLV、MKV等。
   - **适用情境：** 无论是在线播放还是本地播放，都需要进行解封装，因为音视频文件通常以特定的封装格式存储。

3. **解码：**

   - **定义：** 解码是将已经提取的音频和视频数据进行解码，还原成原始的音频和视频信号。这一步骤需要使用相应的解码器，如H.264解码器用于视频，AAC解码器用于音频。
   - **适用情境：** 在解封装后，得到的音视频数据需要解码才能被播放，因此无论是在线播放还是本地播放都需要解码。

4. **音视频同步：**

   - **定义：** 音视频同步是确保音频和视频在播放过程中保持同步的过程。这涉及到调整音频和视频的播放速度，以确保二者能够在正确的时间点同步呈现，避免出现声音和画面不匹配的情况。

   - **适用情境：** 在解码后，音频和视频的播放速度可能有所不同，因此需要进行音视频同步，以提供良好的观看体验。

   - **补充：**音视频同步通常涉及时间戳（timestamp）的概念。在解码后的音频和视频数据流中，每一帧都会被赋予一个时间戳，用于指示这一帧应该在时间轴上的哪个位置播放。具体的同步机制如下：

     - **时间戳标记：** 在解码阶段，每个音频和视频帧都会附带一个时间戳。这个时间戳表示该帧在整个媒体流中的时间位置。

     - **时钟同步：** 播放系统维护一个全局时钟，用于跟踪当前的播放时间。该时钟可能是硬件时钟，也可能是由软件实现的逻辑时钟。

       - 在多媒体播放系统中，通常会维护两个独立的时钟，一个用于跟踪视频的播放时间，另一个用于跟踪音频的播放时间。

         具体来说：

         1. **视频帧时钟：** 用于跟踪视频帧的播放时间。视频帧按照它们的时间戳排队，视频帧时钟会不断地更新，以指示当前应该播放的视频帧是什么。视频帧时钟通常是与图形处理单元（GPU）或其他硬件相关联的硬件时钟。
         2. **音频帧时钟：** 用于跟踪音频帧的播放时间。音频帧按照它们的时间戳排队，音频帧时钟不断更新，指示当前应该播放的音频帧是什么。音频帧时钟通常是与音频输出设备相关联的硬件时钟。

     - **时钟同步调整：** 当播放器开始播放时，视频帧和音频帧按照它们的时间戳排队。如果视频和音频时钟之间存在差异，播放系统会动态调整其中一个时钟，以确保音频和视频的同步。这可能包括拉伸或压缩其中一个时钟，使它们保持同步。

     - **缓冲管理：** 为了保持音视频同步，播放系统通常使用缓冲区来处理不同速度的音频和视频帧。缓冲区允许更灵活地管理时间戳，确保适当的同步调整。

     - **丢帧和插帧：** 在极端情况下，如果音频和视频的同步无法通过时钟调整来维持，播放系统可能会选择丢弃一些视频帧或插入一些额外的帧，以尽量保持同步。

     通过这种时间戳和时钟同步的机制，播放系统可以有效地协调音频和视频的播放，提供给用户一个同步的、流畅的多媒体播放体验。

5. **播放：**

   - **定义：** 播放是最后一步，将已解码且同步的音频和视频信号交给音频和视频设备进行实际播放。这可能包括将视频图像显示在屏幕上，同时通过扬声器播放音频。

   - **适用情境：** 无论是在线播放还是本地播放，最终目标都是将音视频内容呈现给用户。

   - **补充：**在播放这个步骤，解码后的音视频流需要被传递给操作系统或特定的多媒体框架，以便最终呈现在用户界面上。整个过程涉及到图像显示和声音输出两个主要方面：

     1. **视频处理：**
        - **图像渲染：** 解码后的视频帧通常以图像的形式存在。这些图像帧需要被传递给图形处理部分，通常是操作系统提供的图形接口或特定的图形库。这样，图像可以被绘制到显示器上，实现视频的可视化呈现。
        - **硬件加速：** 为了提高视频渲染性能，可以使用硬件加速技术，如图形处理单元（GPU）来处理图像的渲染和显示。
     2. **音频处理：**
        - **音频输出：** 解码后的音频数据需要传递给声音处理系统。通常，操作系统或音频库提供了接口，允许将音频数据发送到音频输出设备，例如扬声器。
        - **音频同步：** 在播放过程中，确保音频和视频的同步是关键的。这可能需要对音频数据进行缓冲管理，以协调音频和视频的播放速度。

     综合而言，播放阶段依赖于操作系统或多媒体框架来管理图像和音频的输出。在现代计算机系统中，通常会使用专门的 API（如DirectX或OpenGL用于图形，以及WASAPI、Core Audio或ALSA用于音频）来处理这些任务。硬件加速也可以利用现代硬件的图形处理单元（GPU）和音频处理单元（DSP）来提高播放性能。这样的设计确保了音视频播放的高效性和兼容性。









## 科普小知识

### 视频如何通过视频编码再到视频格式封装的

视频编码是**将原始视频信号转换成数字信号**，并**通过压缩算法来减少数据量**，以便于存储和传输。常见的视频编码标准包括H.264、H.265、MPEG-2等。

在视频编码完成后，需要**将编码后的视频信号打包成特定的格式**，即视频封装格式。视频封装格式通常包括了**音频、视频、字幕、章节**等多个流的封装，以便于**在播放时同时进行多个流的同步播放**。常见的视频封装格式包括AVI、MKV、MP4、MOV等。

在视频编码之前，需要对视频进行采集和处理。视频采集通常是通过摄像头或其他视频设备将原始视频信号转换成数字信号。视频处理则包括对视频进行**去噪、颜色校正、降低码率**等操作，以便于视频编码器对视频进行压缩。

总的来说，视频编码和视频格式封装是视频处理和传输的重要组成部分。视频编码负责将原始视频信号压缩为较小的数据量，而视频格式封装则将编码后的视频流打包为特定的格式，以便于存储和传输。



### 一般进行录像生成的视频，也是上述流程吗

是的，一般进行录像生成的视频也会经历类似的流程。在录制视频时，摄像机或手机摄像头等设备会对视频进行采集和压缩编码，得到视频的编码数据。这些编码数据需要根据特定的封装格式进行打包，包含视频的音频、字幕、章节信息等元数据，形成最终的视频文件。不同的设备和应用程序可能会使用不同的视频编码器和封装格式，但是视频的基本处理流程是相似的。



### m3u8到底是什么格式

#### **视频播放的过程**

现在的视频网站采用的是流媒体传输协议，就是将一段视频切成无数个小段，这几个小段就是ts格式的视频文件，一段一段的网站上播放。

这样做的好处是观看更加流畅，因为他会根据网络状况自动切换视频的清晰度，在网络状况不稳定的情况下，对保障流畅播放非常有帮助。

我们可以了解下，一个视频播放的全过程。

> 1.服务器采集编码传输视频到切片器
> 2.切片器对视频创建**索引文件，**并且**切割**成**n个ts文件**
> 3.这2个文件传输到http服务器上
> 4.网站/客户端根据**索引文件**查找http服务器上的**ts文件**，连续播放这n个ts文件，就可以了。

给大家画了下流程图



![img](https://pic4.zhimg.com/80/v2-53bdb5ade54140de034973bc1bb0a38f_1440w.webp)



所以我们可以知道，索引文件非常重要，索引文件里面存储着ts文件的网络url链接，网站需要拿到索引文件，去按照url链接下载在http服务器中的ts文件，类似于爬虫。

拿到了ts文件之后，本身这些ts文件就是原视频中的一小段视频，所有ts文件下载顺序播放，就完成了整个视频的播放。

而索引文件就是m3u8文件。

现在大部分视频网站传输都是采用这种方法，所以，也就是说，如果你在观看网页视频的时候，能够弄到加载该视频的m3u8文件，那么再配合一些工具，就能下载该视频了。

该工具的作用就类似于视频网站,能够根据索引文件去下载ts文件。

更具体的耍耍m3u8文件，可参考 [【全网最全】m3u8到底是什么格式？一篇文章搞定m3u8下载](https://zhuanlan.zhihu.com/p/346683119)





#### 为什么有无损音频格式而没有无损视频压缩格式

刻意追求无损就是不讲科学的人为了求个心理安慰。

立论：视频和音频都是人类欺骗自己感官的一种手段。

一段五秒钟的视频或者音频，有可能占多大的存储空间？

答案是要多大有多大，因为这是由你的**采样质量**和**采样频率**共同决定的。

对声音和动画，不存在无损的保留。

更详细可见：[为什么有无损音频格式而没有无损视频压缩格式？](https://www.zhihu.com/question/27889429)





### 码流

码流（Bitstream）指的是一段**二进制数据流**，它包含了被编码后的数字信号，例如视频、音频、图像等等。在数字通信、数字媒体、计算机网络和信息安全等领域中，码流是一个很重要的概念。

在视频和音频领域，码流通常是通过视频编码和音频编码技术将视频和音频信号压缩成数字信号的一种形式。码流中的每一个二进制数据都代表着一些具体的信息，例如视频中的像素信息、色彩信息、帧率、码率等等，音频中的采样率、采样位数、声道数、码率等等。码流的数据量一般是通过码率来描述的，单位通常是每秒的比特数（bps）或千比特数（Kbps）。

码流的应用非常广泛，例如视频传输、视频会议、流媒体、网络广播、数字电视等等，**码流的质量和稳定性对于实时传输和播放的效果有很大的影响**。因此，在处理和传输码流时，需要注意信号的带宽、延迟、抖动等问题，以保证码流的正确性和稳定性。





#### 码流和码率的区别

码流和码率在某些情况下可以视为相同的概念，但它们**并不完全等同**。

简单来说，码流（Bitstream）是指一段连续的二进制数据流，包含了被编码后的数字信号，例如音频、视频、图像等等。而码率（Bitrate）则是指每秒钟传输的比特数，也可以理解为**码流的传输速率**。码率是码流传输中一个非常重要的指标，通常用于描述一个视频或音频流的传输速率和质量。

例如，一部高清视频文件的码流是20Mbps，这个码流中包含了视频和音频信号的所有信息。如果该视频的播放时间为1小时，则其对应的码率为：

20Mbps / 8 bits/byte = 2.5MB/s 
2.5MB/s * 60s/min * 60min/hour = 9GB/hour

这意味着，该视频在播放期间每小时会传输9GB的数据量，这个码率越高，代表传输速率越快，同时也意味着需要更大的带宽和存储空间来传输和保存这个码流。因此，在实际应用中，需要根据具体的场景和要求来控制码率的大小，以达到最优的传输效果。





### 视频压缩比

视频压缩比（Compression Ratio）是指**在压缩前后视频文件的大小比例**。具体来说，它是指压缩前的视频文件大小与压缩后文件大小的比值，通常用百分比表示。

例如，如果一个视频文件的原始大小为500MB，经过压缩后，文件大小缩小到了100MB，则该视频的压缩比为：

**压缩比 = 原始文件大小 / 压缩后文件大小** = 500MB / 100MB = 5:1

因此，该视频的压缩比为5:1，也就是说，压缩后的文件大小只有原始文件大小的五分之一。在实际应用中，视频压缩比通常越高，代表压缩效果越好，同时也意味着压缩后的文件大小更小，传输和存储成本更低。

然而，需要注意的是，视频压缩比并不是越大越好或越小越好。如果压缩比太高，会导致视频质量严重下降，出现模糊、失真、花屏等问题，影响观感和应用效果。因此，在实际应用中，需要根据具体的场景和需求来平衡视频压缩比和视频质量，以达到最优的压缩效果。





### 多码流技术

多码流技术指的是在视频压缩编码的过程中，**同时产生多个不同码率、分辨率或编码质量的视频流**。这些流可以在不同的网络环境下进行自适应码流调节，以保证视频在不同的网络条件下的播放质量和稳定性。这一技术被广泛应用于网络直播、视频点播等多媒体传输领域。

在多码流技术中，主要有以下几种类型的码流：

1. 分辨率码流：以不同的分辨率为区分，如标清、高清等。
2. 码率码流：以不同的码率为区分，常用于网络直播中。低码率适用于网络条件较差的情况，而高码率则适用于网络条件较好的情况。
3. 画质码流：以不同的画质为区分，常用于视频点播和网络直播中。通过在不同的画质和帧率之间自适应调节，可以在不影响视频质量的情况下提高视频播放效率。

通过多码流技术，视频传输可以更好地适应不同的网络环境，保证视频播放的质量和稳定性，提高用户的观看体验。
