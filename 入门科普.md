# 音视频入门科普





## 音视频背景

ISO：国际标准化组织，官方网址：[ISO - International Organization for Standardization](https://www.iso.org/home.html)。很多音视频协议都可以从这里找到。

**MPEG**：ISO与IEC下属的针对运动图像与语音压缩制定国际标准的组织，全称为运动图像专家组(Moving Picture Experts Group)，官网网址：[https://mpeg.chiariglione.org](https://mpeg.chiariglione.org/)。

IETF：互联网工程任务组(Internet Engineering Task Force)，官方网址：[https://www.ietf.org](https://www.ietf.org/)。常见的网络协议、多媒体通信协议的地址：https://tools.ietf.org/html/。

### MPEG发展历史

| 名称   | 编号         | 年份 | 描述                                            |
| ------ | ------------ | ---- | ----------------------------------------------- |
| MPEG1  | ISO/IEC11172 | 1992 | 用于存储语音的编码，MPEG1 audio layer3，简称mp3 |
| MPEG2  | ISO/IEC13818 | 1994 | 用于数字电视、图像通信的编码                    |
| MPEG4  | ISO/IEC14496 | 1998 | 用于视频电话、家庭影音的编码                    |
| MPEG7  | /            | 1999 | 生成一种用来描述多媒体内容的标准，MPEG7=1+2+4   |
| MPEG21 | /            | 1999 | 多媒体框架                                      |



## 视频



### 码率

码率，又叫比特率，单位时间内传输的数据量，单位一般为kbps(千位每秒)。需要注意的是，这里b代表bit，而不是byte。
计算公式：

- 平均码率(kbps)=文件大小(kb) * 8 / 时间(s)
- 动态码率(kbps)=每秒传输数据量(kb) * 8



恒定码率：CBR，码率稳定可控，带宽要求不高，图像变化量比较大时**方块效应比较明显**。

动态码率：VBR，码率波动较大，带宽要求较高，图像变化量比较大时方块效应有所改善。发生网络抖动时，比较容易丢包，需要重传，或者FEC前向纠错，从而**带来延时**。



### 分辨率

分辨率又称为解析度，分辨率越高，像素越多，图像越清晰。

视频分辨率：又称为图像分辨率，由视频的宽高组成，表示形式**宽x高**，常见的视频分辨率有**480P、720P、1080P、2K(2048x1080/2160x1440)、4K(4096x2160/3840x2160)**，具体如下表1所示。

屏幕分辨率：又称为显示分辨率，描述屏幕分辨率的单位是**ppi(pixel per inch，每英寸的像素数)**。因此显然，屏幕分辨率在视频分辨率基础上增加了对实际屏幕的限定。

位分辨率：又称为位深(BitDepth)，每个像素点存储信息的位数。常见的有：8位、16位、24位、32位色彩。Android的Bitmap常见的有ALPHA_8、RGB_565、ARGB_4444、ARGB_8888。

| 显示模式 | 水平像素x垂直像素 | 宽高比 |
| -------- | ----------------- | ------ |
| QCIF     | 176x144           | 11:9   |
| QVGA     | 320x240           | 4:3    |
| CIF      | 352x288           | 11:9   |
| nHD      | 640x360           | 16:9   |
| VGA      | 640x480           | 4:3    |
| HD       | 1280x720          | 16:9   |
| Full HD  | 1920x1080         | 16:9   |
| 2K(FHD+) | 2048x1080         | 17:9   |
| 4K(UHD)  | 3840x2160         | 16:9   |



#### DPI与PPI

##### 概念问题

不论是DPI还是PPI，实际都是一种**换算的概念**，即将图片承载的信息换算为现实中的图片（即人眼能实际看到的图像）。DPI和PPI的区别在于换算的途径不同，DPI面向的是**印刷受体**，而PPI面向的是**荧幕**。



##### PPI

**PPI**是英文`Pixels Per Inch`的缩写，意为**像素每英寸**。英寸是常用的长度单位，大约相当于2.54厘米。而**像素是专用于荧幕的概念**，指的是**荧幕可以解析的最小的点**。因此，PPI值得是像素在荧幕上的密度，PPI越高图像就越清晰。
举例来说，如果电脑屏幕是2K分辨率，即1920×1080像素，它的图像宽为1920像素。而如果这个电脑屏幕的物理宽度是19.2英寸，电脑屏幕是**分辨率就是1920/19.2=100PPI**。



##### DPI

**DPI**是英文`Dots Per Inch`的缩写，意为**点每英寸**。英寸还是那个英寸，但是点的意义有很多。一般来讲，你可以把Dot理解为**取样点**，即**物理设备可以解析的最小单位**。在印刷时，它就可以作为**印刷网点**，而在鼠标等电子设备上，可以理解为**最小操作阈值**（即设备会把多么远的两个点当作一个点来处理）。
我们仍然拿1920×1080像素的图片来举例子，如果印刷设备的解析能力刚好是100DPI，而且你要印制的纸张尺寸刚好是19.2英寸，那么印刷设备就可以刚好把一个像素作为一个取样点，印刷完成后图片的**保真度是百分之百**（也就是图片所有的视觉信息都被印刷出来了）。在大多数情况下，这几个数值都不那么整好，因此保真度会产生损失。



##### 图片内置的DPI和PPI

图片在计算机（或其他设备）里是一系列代表视觉信息的数据，它的单位是像素。因此，真正能定义图片尺寸的是分辨率，比如前面提到的1920×1080像素。

而很多格式的图片会内置DPI或PPI这个属性，它的唯一作用是作为图形处理软件的参考值。比如，一张图片的PPI是300，那么置入Illustrator的时候就会直接是300PPI下的尺寸。DPI是完全相同的道理。换言之，不论图片的DPI和PPI如何变化，如果**分辨率不变**，那么图片承载的信息量就不会变化，在**实际意义上图片的“大小”都是相同的**。



##### 设备的DPI和PPI

我们之前提到了印刷设备的解析能力这个问题。其实**每个荧幕和每个印刷设备都有自己PPI或DPI参数**。我们拿荧幕来说，荧幕的PPI就决定了荧幕的解析能力（注意，并非最大解析能力，而是**绝对解析能力**）。

如你把某个图片的尺寸在屏幕上放大缩小，它的物理尺寸在改变，因此对信息量来说PPI也在改变。然而，荧幕会按照它自己的PPI显示能力来重新解析这张图片，最终形成你肉眼看到的结果。这个过程，我们可以称为“**栅格处理**”。



##### 栅格处理

屏幕的栅格处理指的是将屏幕上的图像、文本等内容按照一个规定的栅格进行处理的过程。在计算机图形学和计算机视觉中，屏幕通常被划分为一个个的像素，每个像素可以看作是屏幕上的一个小方格，这些小方格构成了屏幕的栅格。在这个栅格上，可以对每个像素的颜色、亮度等属性进行编码和存储，从而呈现出图像、文字等视觉内容。栅格处理常常涉及到像素的放大、缩小、旋转、变形等操作，以及像素颜色的调整、滤波、插值等处理，以达到预期的视觉效果。
实际就是将图片**在物理尺寸不变**的情况下，**对DPI或PPI进行调整**，图片的信息量会受到影响。图像为何需要进行栅格处理？因为任何设备都有固定的解析能力，比如很多荧幕的解析能力是72PPI，这时一张全屏后（即在屏幕的物理尺寸下）从信息量上来说有300PPI的图片显然超过了荧幕的解析能力，因此对荧幕来说这么大的图片是没必要的，把图片在荧幕的物理尺寸下处理为72PPI就刚好了。
**几乎所有的栅格都是有损处理，除了某些算法中的整数倍放大。**



### **视频帧**

帧是视频的一个基本概念，表示一幅画面，一段视频由许多帧组成。



### **帧率**

视频帧率：**显示帧数的量度**，单位为**每秒显示帧数**(**FPS**，全称为`Frame Per Second`)。一般视频帧率为24fps，**P制**(PAL，德国提出，中国、印度、巴基斯坦等国家使用)为**25fps**，也就是每帧显示40ms，**N制**(NTSC，美国标准委员会提出，美国、日本、韩国等国家使用)为**30fps**。有些超高帧率的视频达到**60fps**。

常见帧率：

29.97 f/s : 1s 30000/1001帧

24 f/s  或 25 f/s ：1s 24 或 25 帧， 一般的电视/电影帧率

30 f/s  或 60 f/s ：1s 30 或 60 帧， 游戏中30帧可以接受，60帧会感觉非常流畅

一般来讲，85 f/s 以上人眼基本上无法察觉出过度画面，所以过高的帧率在普通游戏视频里没有太大意义。

显示帧率：以帧为单位的位图图像连续出现在显示器的频率，也称为**刷新速率**。通常来说，**人眼能够感知到的最低帧率是每秒 24 帧**，也就是每帧的时间大约是 **42 ms**。如果帧率低于这个值，就会出现卡顿和延迟的感觉，画面也会变得不流畅。在某些高速运动场景中，甚至需要更高的帧率才能保证画面的流畅度和清晰度，比如说游戏或者体育直播等领域，此时一般会追求更高的帧率来提供更好的用户体验。
Android设备刷新率一般为60Hz，也就是帧率为60fps，**每帧为16ms**，由于交互原因，**超过16ms能给人的肉眼带来延迟卡顿的感觉**。做性能优化方面，也就是保证从测量、布局、绘制、上传指令、与GPU交换缓冲区等一系列动作在16ms完成。Android11支持120Hz的更高帧率，一般为对帧率要求极高的应用场景，比如互动游戏。



### **像素格式**

像素格式：像素色彩分量的排列，由每个像素使用的总位数以及各分量的位数决定。图像的像素格式一般是RGBA四个分量通道各占8bits，组成一个32位的像素。其中R代表Red、G代表Green、B代表Blue、A代表Alpha。但是，视频压缩存储的像素格式不是RGBA，而是YUV，其中Y代表亮度(Luma)，U代表色度(Chroma)，V代表对比度(Contrast)。

#### 像素通道

在数字图像中，每个像素都是由数值表示的，这些数值代表了图像上对应点的颜色和亮度信息。因此，图像的本质是由一个个像素点组成的。每个像素点都包含了颜色和亮度等信息。

然而，对于彩色图像而言，每个像素点不仅仅包含了一个颜色信息，而是包含了红、绿、蓝三种颜色信息的组合。这种组合被称为“**颜色通道**”，而包含多个颜色通道的图像就是多通道图像。例如，RGB图像就是一种三通道图像，其中每个像素包含了红色通道、绿色通道和蓝色通道的信息。还有其他的多通道图像格式，例如RGBA图像、CMYK图像等。

多通道图像能够提供更加丰富的颜色信息，因此在一些颜色重要的应用场景下得到了广泛的应用，例如数码相机、电影、电视、游戏等。

> 因此所谓像素通道，并不是说一张图是有多层叠加出来的，而是每个像素点包含多个信息位。





### **画质**

画质：画面质量，由清晰度、锐度、解析度、色彩纯度、色彩平衡等指标构成。(偏主观的概念)

清晰度：指图像细节纹理及其边界的清晰程度。

锐度：反应图像平面清晰程度，以及图像边缘的锐利程度。

解析度：指像素点的数量，与分辨率对应，分辨率越高，解析度越高。

色彩纯度：指色彩的鲜艳程度。所有色彩都是三原色组成，其他颜色都是三原色混合而成，理论上可以混合出256种颜色。原色的纯度最高。**色彩纯度是指原色在色彩中的百分比**。

色彩平衡：用来控制图像的色彩分布，使得图像整体达到色彩平衡。



#### 关于三原色

三原色是指无法再被分解，也不能由其他颜色混合而成的三种颜色。根据应用场景的不同，三原色可以分为两类：叠加型的三原色和消減型的三原色。

**叠加型**的三原色是**红色、绿色和蓝色**，也称为**RGB**。它们用于电视机、投影仪等显示设备。这三种颜色可以混合产生其他颜色，例如红色与绿色混合可以产生黄色或橙色，绿色与蓝色混合可以产生青色，蓝色与红色混合可以产生紫色或品红色 。

**消減型**的三原色是**桃红色、黄色和青色**，也称为**CMY**。它们用于书本、杂志等的印刷。





### **色域与HDR**

色域：指某种表色模式所能表达的**颜色构成的范围区域**，色域空间越大，所能表现的颜色越多。

HDR：High Danamic Range，高动态范围，比普通图像提供更多动态范围和图像细节，能够更好反应真实环境的视觉效果。颜色值经过归一化后，范围一般是[0,1]。而HDR可以表达超出1的颜色值，拥有更大的颜色范围。



### **旋转角度**

旋转角度：视频的YUV储存方向。一般的视频旋转角度是0°，对应的是横屏显示。后置摄像头竖屏拍的视频，旋转角度为90°，对应的是竖屏显示。Android中可以通过*MediaMetaDataRetriever*获取旋转角度。





### YUV

YUV是一种颜色编码系统，其中“Y”代表亮度（Luma），而“U”和“V”代表色度（Chroma）。YUV通常用于**数字视频和电视系统**中，其目的是将图像分成亮度和色度两个分量，从而在**保持图像质量的同时，减小图像数据的大小，更容易地传输和存储图像。**

其中，**亮度指的是图像的明暗程度，也就是灰度值**；而**色度指的是图像中颜色的饱和度和色调**。在YUV编码中，亮度和色度分量都是由原始RGB图像通过线性变换得到的。其中，Y分量代表了图像的亮度信息，而U和V分量则代表了色度信息。

在YUV编码中，Y分量占据了图像数据的大部分，而U和V分量则只占据了极少的空间。这样一来，通过**将图像数据压缩为YUV格式，可以极大地减小数据量**，从而更容易地传输和存储图像。同时，YUV编码还能够对图像数据进行压缩和解压缩，这在数字视频和电视系统中尤为重要。

总之，YUV编码系统通过将图像分成亮度和色度两个分量，可以在保持图像质量的同时，减小图像数据的大小，从而更容易地传输和存储图像。

> YUV是一种表示彩色视频或图像的颜色空间，其中Y表示亮度（luma），U和V分别表示色度（chroma）。
>
> - Y：表示亮度分量，也称为亮度信息，表示图像的明亮度或灰度值，决定了图像的明暗程度。在YUV颜色空间中，Y分量与RGB颜色空间中的R、G、B分量之间有如下的线性关系：Y = 0.299R + 0.587G + 0.114B。
> - U：表示蓝色色度分量，也称为色度差（chroma difference）分量，用于表示图像中蓝色与亮度之间的偏差。在YUV颜色空间中，U分量与RGB颜色空间中的R、G、B分量之间有如下的线性关系：U = -0.14713R - 0.28886G + 0.436B。
> - V：表示红色色度分量，也称为色度差分量，用于表示图像中红色与亮度之间的偏差。在YUV颜色空间中，V分量与RGB颜色空间中的R、G、B分量之间有如下的线性关系：V = 0.615R - 0.51498G - 0.10001B。
>
> YUV常用于视频编码和传输，因为人眼对亮度的敏感度比对色度的敏感度更高，所以可以通过对色度分量进行降采样来减小视频数据的大小，从而节省带宽和存储空间。



#### **YUV的数据格式是如何呢？**

YUV有两种分类方式，即“空间-间”和“空间-内”。“空间-间”的划分方式主要体现在Y、U、V的比例不同；“空间-内”的划分方式主要体现在Y、U、V的比例一定，存储格式不同。



#### **YUV“空间-间”的数据划分**

YUV按照“空间-间”的划分方式，分为YUV444、YUV422、YUV420，如下所示，假设图像为1920*1080：

![](C:\Users\Brain\Desktop\音视频图\YUV1.png)



#### **YUV“空间-内”的数据划分**

YUV按照“空间-内”的划分方式，主要分为packet、planar、semi-planar三种：

◆ packet：打包格式，即先存储一个yuv，再存储下一个yuv；

◆ planar：平面格式，即先存储y平面，再存储u平面，再存储v平面；

◆ semi-planar：先存储y平面，再存储uv平面；



◆ YUV422各种存储格式如下：

![](C:\Users\Brain\Desktop\音视频图\YUV2.png)

◆ YUV420各种存储格式如下：

![](C:\Users\Brain\Desktop\音视频图\YUV3.png)

针对上图中的NV12、NV21、NV16、NV61说明：

◆NV：NV系列都属于semi-plane系列，“12”、“16”代表先U后V，“21”、“61”代表先V后U

◆ 12、16：代表一个像素占的位数



**4.YUV和RGB**

RGB：即red，green，blue三色存储空间，因音视频主要用的是YUV的色彩空间，感兴趣的小伙伴可以拓展下RGB相关知识，本文不再详述。介绍下RGB和YUV的转换公式：

◆ RGB 转 YUV：

  Y = 0.299R + 0.587G + 0.114B

  U= -0.147R - 0.289G + 0.436B

  V = 0.615R - 0.515G - 0.100B

◆ YUV 转 RGB：

  R = Y + 1.14V

  G = Y - 0.39U - 0.58V

  B = Y + 2.03U





### **时长**

视频所有图像播放所需要的时间称为视频时长。计算公式：**时长(s)=帧数x每帧时长=帧数x(1/帧率)**。假设一个视频帧数为1000，帧率为25fps，那么时长为40s。





### 视频封装格式

首选需要弄清楚的概念是：视频封装格式是**将视频编码数据和音频编码数据以及其他元数据<u>进行封装的文件格式</u>**。视频格式通常由一个容器格式和一个或多个音视频编码格式组成。**容器格式(即封装格式)负责存储和传输数据**，而**音视频编码格式则负责对音频和视频数据进行压缩和编码**。

视频的封装格式，由特定格式头+媒体信息+音视频轨(字幕)数据+视频轨索引组成。常见的封装格式有：mp4、mkv、webm、avi、3gp、mov、wmv、flv、mpeg、asf、rmvb等。



> 常见的视频封装格式包括：
>
> 1. AVI(Audio Video Interleave)：是由微软公司开发的视频封装格式，最早出现在Windows 3.1操作系统中，支持多种视频和音频编码格式。由于不支持流媒体传输，目前使用较少。
> 2. MP4(MPEG-4 Part 14)：是一种常用的视频封装格式，可支持多种音视频编码格式，例如H.264、AAC等，广泛用于互联网视频流媒体传输。
> 3. MKV(Matroska)：是一种开源的视频封装格式，可以容纳多种不同编码方式的音频、视频和字幕轨道。MKV支持高清视频和多语言音轨等特性，也被广泛用于互联网视频流媒体传输。
> 4. MOV(QuickTime File Format)：是由苹果公司开发的视频封装格式，支持多种视频和音频编码格式，也支持流媒体传输。常用于苹果设备和软件中。
> 5. FLV(Flash Video)：是一种Adobe公司开发的视频封装格式，适用于Adobe Flash Player播放器，支持流媒体传输，常用于网络视频传输。



### 视频编码格式/协议

视频经过解封装得到的视频轨数据，是经过编码的，所以**显示视频帧前需要解码**。不同编码算法组成不同编码协议，常见的有：H264(AVC，一般使用x264编码)、H265(HEVC，一般使用x265编码)、VP8、VP9、MPEG4、MJPEG、WMV3等。

#### 注！

**视频封装格式一般不包含视频的编码格式**，封装格式主要是用来组织和存储视频和音频数据，以及描述这些数据的元数据信息，如视频分辨率、帧率、音频采样率、编码格式、时间戳等等。

编码格式则是指将视频和音频等原始数据进行压缩编码的格式，如H.264、H.265、MPEG-2、MPEG-4、VP9等等。编码格式的作用是将原始数据压缩成较小的体积，以便于存储和传输。在视频封装格式中，一般会使用压缩后的视频和音频数据，而不是原始的未经过压缩编码的数据。因此，**视频封装格式和视频编码格式是两个不同的概念**，但**它们经常一起使用，以实现视频的存储和传输**。



#### H.264

##### **什么是H.264**

H.264是由ITU-T视频编码专家组（VCEG）和ISO/IEC动态图像专家组（MPEG）联合组成的联合视频组（JVT，Joint Video Team）提出的高度压缩数字视频编解码器标准。



##### 特点

1. **高压缩效率：** H.264采用了先进的压缩技术，包括运动补偿、帧内预测、变换和量化等，以提高视频压缩效率。这使得相对较小的比特率就能够保持相对高的视频质量。
2. **支持多种分辨率和帧率：** H.264支持多种分辨率和帧率设置，适用于不同的应用场景，从低分辨率的移动设备视频到高分辨率的高清电视。
3. **网络友好：** H.264在视频传输中具有较好的网络适应性。它能够在不同的网络带宽条件下提供稳定的视频传输，适应网络波动和丢包等情况。
4. **广泛应用：** H.264已经成为许多应用领域的标准，包括视频会议、实时视频传输、数字电视广播、互联网视频流媒体等。许多设备和平台都支持H.264编解码，使其成为一种通用的视频编码标准。
5. **提供多个配置和级别：** H.264标准定义了多个配置和级别，以满足不同应用的需求。这些配置和级别包括基本编码配置、主要编码配置、高级编码配置等。



##### **H.264的数据格式是怎样的**

H.264由视频编码层（VCL）和网络适配层（NAL）组成。

◆ VCL：H264编码/压缩的核心，主要负责将视频数据编码/压缩，再切分。

◆ NALU = NALU header + NALU payload



##### **VCL是如何管理H264视频数据**

◆ 压缩：预测（帧内预测和帧间预测）-> DCT变化和量化 -> 比特流编码；

◆ 切分数据，主要为了第三步。"切片(slice)"、“宏块(macroblock)"是在VCL中的概念，一方面提高编码效率和降低误码率、另一方面提高网络传输的灵活性。

◆ 包装成『NAL』。

◆ 『**VCL**』最后会被包装成『**NAL**』



##### **NAL头的数据结构体**

![](C:\Users\Brain\Desktop\音视频图\h.264-1.png)

◆ F（forbidden_zero_bit）：1 位，初始为0。当网络识别此单元存在比特错误时，可将其设为 1，以便接收方丢掉该单元

◆ NRI（nal_ref_idc）：2 位，用来指示该NALU 的重要性等级。值越大，表示当前NALU越重要。具体大于0 时取何值，没有明确规定

◆ Type（nal_unit_type）：5 位，指出NALU 的类型，如下所示：

<img src="C:\Users\Brain\Desktop\音视频图\h264-2.png" style="zoom: 67%;" />



##### **H.264码流结构**

◆ H.264 = start_code + NALU（start_code：00000001 or 000001）

![](C:\Users\Brain\Desktop\音视频图\h264-3.png)



◆ 每个NAL前有一个起始码 0x00 00 01（或者0x00 00 00 01），解码器检测每个起始码，作为一个NAL的起始标识，当检测到下一个起始码时，当前NAL结束。

◆ 同时H.264规定，当检测到0x000000时，也可以表征当前NAL的结束。那么NAL中数据出现0x000001或0x000000时怎么办？H.264引入了防止竞争机制，如果编码器检测到NAL数据存在0x000001或0x000000时，编码器会在最后个字节前插入一个新的字节0x03，这样：

0x000000－>0x00000300

0x000001－>0x00000301

0x000002－>0x00000302

0x000003－>0x00000303



##### **I帧、P帧和B帧**

提到H.264，不得不提I帧、P帧、B帧、IDR帧、GOP。

◆ I帧（Intra-coded picture，帧内编码图像帧），表示关键帧，采用类似JPEG压缩的DCT(Discrete Cosine Transform，离散余弦变换)压缩技术，可达1/6压缩比而无明显压缩痕迹；

◆ P帧（Predictive-coded picture，前向预测编码图像帧），表示的是跟之前的一个关键帧或P帧的差别，P帧是参考帧，它可能造成解码错误的扩散；

◆ B帧（Bidirectionally predicted picture，双向预测编码图像帧），本帧与前后帧（I或P帧）的差别，B帧压缩率高，但解码耗费CPU；

◆ IDR帧（Instantaneous Decoding Refresh，即时解码刷新）：首个I帧，是立刻刷新,使错误不致传播，IDR导致DPB（DecodedPictureBuffer参考帧列表——这是关键所在）清空；在IDR帧之后的所有帧都不能引用任何IDR帧之前的帧的内容；IDR具有随机访问的能力，播放器可以从一个IDR帧播放。

◆ GOP（Group Of Picture，图像序列）：两个I帧之间是一个图像序列，一个GOP包含一个I帧



##### **解码时间戳和显示时间戳**

当然，H.264中还有两个重要的概念：DTS和PTS

◆ DTS（Decoding Time Stamp，解码时间戳解）：读入内存中的比特流在什么时候开始送入解码器中进行解码

◆ PTS（Presentation Time Stamp，显示时间戳）：解码后的视频帧什么时候被显示出来

![](C:\Users\Brain\Desktop\音视频图\h264-4.png)



## 音频



### 采样率

对声音信号**每秒的采样次数**，**采样率越高，声音的还原越真实**。采样率单位为Hz，常见的采样率有：8000Hz、16000Hz、44100Hz、48000Hz。人类一般能够听到的声音范围：20Hz～20000Hz。根据奈奎斯特采样定理：**当采样频率大于信号中最高频率的2倍时，采样后的数字信号能够完整保留原始信号的信息。**






### **声道**

指声音在录制或播放时，**在不同空间位置采集或回放的相互独立音频信号**。声道数指在录音时的**音源数量**，或者在播放时的扬声器数量。





### 声道布局

不同声道数对应不同声道布局。常见的声道布局有**单声道(mono)、立体声道(stereo)、四声环绕、5.1声道(杜比音效)**。

单声道：只有一个声道，优点数据量小，amr_nb和amr_wb默认为单声道，缺点是<u>缺乏对声音位置定位</u>。

立体声道：一般为两个声道，由<u>左声道、右声道组成，改善对声音位置定位的状况</u>。

四声环绕：由<u>前左、前右、后左、后右</u>组成，形成立体环绕。4.1声道是在四声环绕基础上，<u>增加一个低音</u>。

5.1声道：<u>在4.1基础上，增加一个中场</u>声道，杜比AC3就是采用5.1声道，也**就是影院宣传的杜比音效**。

### 音质

音质：声音的质量，经过编码压缩后的**音频信号保真度**，**由音量、音高和音色组成**。

音量：音频的强度，数值范围0-100，静音时为0，最大值为100。Android中有提供音量增强LoudnessEnhancer，调节声音分贝值。

音高：声音的音调，即音频频率或每秒变化次数。

音色：音频泛音，又称为音品，不同声音表现在波形方面与众不同的特性。





### 音频封装格式

音频的封装格式，与视频封装格式类似，由**特定格式头+媒体信息+音频轨数据**组成。常见的封装格式有：mp3、m4a、ogg、amr、wma、wav、flac、aac、ape等。





### 音频编码格式/协议

音频经过解封装得到的音频轨数据，也是经过编码的。常见的音频编码协议有：mp3、aac、amr_nb、amr_wb、ac3、vorbis、opus、flac、wmav2等。

#### 注

需要说明的还是音频封装格式和编码格式仍然不是同一个东西，而常说的MP3格式以及AAC、FLAC格式，它们既是封装格式也是编码格式，以MP3为例：

MP3既是一种音频编码格式，也是一种音频封装格式。MP3编码格式是一种有损压缩技术，能够将音频数据压缩至较小的文件大小，同时尽可能保持高质量的音频效果。而MP3封装格式则是将MP3编码产生的音频数据进行封装，使其成为一个完整的音频文件，其中包含了音频数据本身、文件头信息以及其他的元数据信息，方便播放器软件读取和解码。因此，MP3是同时具备编码和封装功能的一种音频格式。





### 采样数

采样数，即每帧采样的数量。在**FFmpeg的AVFrame中，定义为nb_samples**。





### 采样位数

采样位数，即**每个采样占用多少位**。在RIFF(Resource Interchange File Format)资源交换文件格式中字段bits_per_sample表示采样位数，在FFmpeg也是用这个字段表示采样位数。

音频采样位数是指在数字音频中用来表示**一个样本的比特数**。它决定了**每个样本的精度和范围**，通常用8位、16位、24位或32位来表示。比特数越大，表示每个采样的精度越高，音频信号的细节也越丰富，但同时也会占用更多的存储空间和带宽。一般来说，16位是目前最为常用的采样位数，可以提供足够的精度和动态范围，同时也具有较高的兼容性和广泛支持。但对于专业音频工程师或高要求的音频应用，可能会使用更高的采样位数以获得更高的音频质量和精度。





### 存储空间

音频的每秒存储空间由：采样率、声道数、每个采样位数。假设采样率为44.1k，声道数为2，采样位数为16。那么，每秒所占存储空间字节数=44100 * 2 * 16 / 8





### 码率

音频中的码率是指一个数据流中每秒能通过的信息量，单位为 b/s ，可以用以下公式计算：

码率 = 采样率 * 采用位数 * 声道数





### 帧时长

音频的帧时长=采样数 / 采样率。假设采样率为44.1k，采样数为1024。那么每帧时长约等于23ms。

音频帧时长也可以通过下面的公式计算：

帧时长 = 帧大小 ÷ 采样率 ÷ 采样位数 ÷ 声道数

其中，帧大小指的是一个音频帧包含的字节数，采样率是指每秒采样次数，采样位数是指每个采样值用多少位二进制表示，声道数是指音频信号的声道数。

例如，一个采样率为44100Hz、采样位数为16位、立体声的音频文件，每个音频帧的大小为4字节，那么该音频帧的时长为：

帧时长 = 4 ÷ 44100 ÷ 16 ÷ 2 = 0.00000226757 秒 (约为2.27毫秒)



### 音频采样格式

音频采样格式指的是在进行模拟信号数字化的过程中，将模拟信号采样后，转换为数字信号所采用的编码格式。

音频的采样格式分大端（big-endian）和小端（little-endian）两种。在大端格式中，高位字节存储在低地址，低位字节存储在高地址；而在小端格式中，低位字节存储在低地址，高位字节存储在高地址。按照符号划分有：有符号与无符号。按照类型划分有：整型与浮点型。按照存储位数划分有：8位、16位、32位、64位，都是8的倍数。

常见的音频采样格式包括有：

1. PCM（Pulse Code Modulation）脉冲编码调制格式：PCM是一种最基本的数字音频编码格式，将模拟音频信号按一定的采样率和量化位数转换成数字信号，可以被大多数音频处理软件所支持。
2. AAC（Advanced Audio Coding）高级音频编码格式：AAC是一种常见的有损压缩音频格式，广泛应用于音乐、电影、电视节目等领域。与MP3相比，AAC编码效率更高，音质更好。
3. MP3（MPEG-1 Audio Layer III）音频压缩格式：MP3是一种有损压缩的音频格式，可以将原始的音频信号压缩至原来的1/10或更小的体积。由于其高压缩率和较好的音质，在网络传输和存储方面得到了广泛的应用。
4. FLAC（Free Lossless Audio Codec）无损压缩格式：FLAC是一种无损压缩的音频格式，与MP3等有损压缩格式相比，它可以保留原始音频信号的所有信息，音质更好，但文件体积也较大。
5. WMA（Windows Media Audio）Windows媒体音频格式：WMA是微软公司开发的一种音频格式，与MP3等格式相比，它可以提供更高的音质和更小的文件体积，因此被广泛应用于网络传输和存储中。









## 音视频编码

### 缘由

为什么需要编码呢？

我们举一个例子：
以一个分辨率为` 1920 * 1080 `像素且帧率为` 30 f/s` 的视频为例，共有 1920 * 1080 = 2073600 像素， 每像素 24b（假设采取RGB24），也就是单帧图片 2073600 * 24b = 49766400b = 6220800B ≈ 6.22MB，那么每秒视频大小 6.22 MB * 30 = 186.6MB，那么一分钟就是11GB，一部`90分钟`的电影，约为 `990GB`

而显然，对于视频，由于画面是逐渐过渡的，因此在整个视频中，包含大量画面/像素的重复，这会**包含大量0和1的重复数据，这正好提供了非常大的压缩空间。**



### 视频编码

主流视频编码有 ITU（International Telecommunication Union，国际电信联盟）的H.26x（1/2/3/4/5）系列、MPEG（Motion Picture Experts Group, 运动图像专家组）的MPEG（1/2/3/4）系列



#### 编码举例

H.264 : H.264 / MPEG-4 第十部分，或称为 AVC（Advanced Video Coding，高级视频编码），是一种视频压缩标准，也是一种被广泛使用的高精度视频的录制、压缩和发布格式。

H.265：是H.264 / MPEG-4 AVC的继承者，HEVC（High Efficiency Video Coding， 高效率视频编码），是一种视频压缩标准，被认为不仅提升了图像质量，同时也能达到H.264 / MPEG-4 AVC两倍的压缩率（等同于同样画面质量下比特率减少了50%），可以支持4K分辨率甚至超高画质电视，最高分辨率可达 8129 * 4320（8K分辨率），它是目前发展的趋势。





### 音频编码

#### 原始数据

音频数据的承载方式最常用的是PCM（**P**ulse-**c**ode **m**odulation，脉冲编码调制）是一种模拟信号的数字化方法。PCM将信号的强度依照同样的间距分成数段，然后用独特的数字记号（通常是二进制）来量化。PCM常用于数字电信系统上，也是电脑和红皮书中的标准形式。



#### 缘由

原始的PCM音频数据包含非常大的数据量(PCM并不流行于诸如DVD或DVR的消费性商品上，因为它需要相当大的比特率（DVD格式虽然支持PCM，不过很少使用）)，因此音频也有了很多种压缩方式WAV、MP3、WMA、APE、FLAC（后两种为音频无损压缩技术）





## 音视频播放原理

### 音视频播放分类

音视频播放可以分为两种情况：在线播放和本地播放。

1. **在线播放：**
   - **定义：** 在线播放是指通过互联网直接从服务器上获取音视频内容，并实时流式传输到用户的终端设备，用户在播放过程中不需要下载完整的文件。
   - **特点：**
     - **即时性：** 用户可以即时访问和播放媒体内容，无需等待完整下载。
     - **节省存储空间：** 无需在本地设备上存储完整的音视频文件，节省了存储空间。
     - **多样化内容：** 可以随时获取各种不同类型的音视频内容。

2. **本地播放：**
   - **定义：** 本地播放是指用户在本地设备上拥有完整的音视频文件，通过本地储存介质（如硬盘、SD卡等）进行播放，无需网络连接。
   - **特点：**
     - **独立性：** 用户不依赖互联网连接，可以在没有网络的情况下随时播放。
     - **高清晰度：** 由于文件完整存在于本地设备上，播放过程中不受网络速度限制，有助于实现高清晰度播放。
     - **自主管理：** 用户可以自由管理和组织本地存储的音视频文件，不受在线服务器存储空间的限制。

总的来说，在线播放适用于需要即时访问、不占用本地存储空间的场景，而本地播放适用于用户希望在没有网络连接或对高清晰度播放有要求的情况。在实际应用中，很多音视频播放平台同时支持在线和本地播放，以满足用户在不同场景下的需求。





### 音视频播放原理

音视频播放的原理主要分为：解协议->解封装->解码->音视频同步->播放。当然，如果是本地播放，没有解协议这一步骤。

1. **解协议：**

   - **定义：** 在音视频播放开始之前，首先要进行解协议的过程。这一步骤涉及到从网络获取音视频数据流，通常使用的是协议，比如HTTP、RTSP等。解协议的任务是建立与服务器的连接，发起请求并接收响应，以获取音视频数据流的位置和相关信息。
   - **适用情境：** 主要在在线播放中发挥作用，因为本地播放不涉及网络传输。

2. **解封装：**

   - **定义：** 解封装是将从网络获取的音视频数据流按照特定的封装格式进行解析，提取出其中的音频和视频数据。常见的封装格式包括MP4、FLV、MKV等。
   - **适用情境：** 无论是在线播放还是本地播放，都需要进行解封装，因为音视频文件通常以特定的封装格式存储。

3. **解码：**

   - **定义：** 解码是将已经提取的音频和视频数据进行解码，还原成原始的音频和视频信号。这一步骤需要使用相应的解码器，如H.264解码器用于视频，AAC解码器用于音频。
   - **适用情境：** 在解封装后，得到的音视频数据需要解码才能被播放，因此无论是在线播放还是本地播放都需要解码。

4. **音视频同步：**

   - **定义：** 音视频同步是确保音频和视频在播放过程中保持同步的过程。这涉及到调整音频和视频的播放速度，以确保二者能够在正确的时间点同步呈现，避免出现声音和画面不匹配的情况。

   - **适用情境：** 在解码后，音频和视频的播放速度可能有所不同，因此需要进行音视频同步，以提供良好的观看体验。

   - **补充：**音视频同步通常涉及时间戳（timestamp）的概念。在解码后的音频和视频数据流中，每一帧都会被赋予一个时间戳，用于指示这一帧应该在时间轴上的哪个位置播放。具体的同步机制如下：

     - **时间戳标记：** 在解码阶段，每个音频和视频帧都会附带一个时间戳。这个时间戳表示该帧在整个媒体流中的时间位置。

     - **时钟同步：** 播放系统维护一个全局时钟，用于跟踪当前的播放时间。该时钟可能是硬件时钟，也可能是由软件实现的逻辑时钟。

       - 在多媒体播放系统中，通常会维护两个独立的时钟，一个用于跟踪视频的播放时间，另一个用于跟踪音频的播放时间。

         具体来说：

         1. **视频帧时钟：** 用于跟踪视频帧的播放时间。视频帧按照它们的时间戳排队，视频帧时钟会不断地更新，以指示当前应该播放的视频帧是什么。视频帧时钟通常是与图形处理单元（GPU）或其他硬件相关联的硬件时钟。
         2. **音频帧时钟：** 用于跟踪音频帧的播放时间。音频帧按照它们的时间戳排队，音频帧时钟不断更新，指示当前应该播放的音频帧是什么。音频帧时钟通常是与音频输出设备相关联的硬件时钟。

     - **时钟同步调整：** 当播放器开始播放时，视频帧和音频帧按照它们的时间戳排队。如果视频和音频时钟之间存在差异，播放系统会动态调整其中一个时钟，以确保音频和视频的同步。这可能包括拉伸或压缩其中一个时钟，使它们保持同步。

     - **缓冲管理：** 为了保持音视频同步，播放系统通常使用缓冲区来处理不同速度的音频和视频帧。缓冲区允许更灵活地管理时间戳，确保适当的同步调整。

     - **丢帧和插帧：** 在极端情况下，如果音频和视频的同步无法通过时钟调整来维持，播放系统可能会选择丢弃一些视频帧或插入一些额外的帧，以尽量保持同步。

     通过这种时间戳和时钟同步的机制，播放系统可以有效地协调音频和视频的播放，提供给用户一个同步的、流畅的多媒体播放体验。

5. **播放：**

   - **定义：** 播放是最后一步，将已解码且同步的音频和视频信号交给音频和视频设备进行实际播放。这可能包括将视频图像显示在屏幕上，同时通过扬声器播放音频。

   - **适用情境：** 无论是在线播放还是本地播放，最终目标都是将音视频内容呈现给用户。

   - **补充：**在播放这个步骤，解码后的音视频流需要被传递给操作系统或特定的多媒体框架，以便最终呈现在用户界面上。整个过程涉及到图像显示和声音输出两个主要方面：

     1. **视频处理：**
        - **图像渲染：** 解码后的视频帧通常以图像的形式存在。这些图像帧需要被传递给图形处理部分，通常是操作系统提供的图形接口或特定的图形库。这样，图像可以被绘制到显示器上，实现视频的可视化呈现。
        - **硬件加速：** 为了提高视频渲染性能，可以使用硬件加速技术，如图形处理单元（GPU）来处理图像的渲染和显示。
     2. **音频处理：**
        - **音频输出：** 解码后的音频数据需要传递给声音处理系统。通常，操作系统或音频库提供了接口，允许将音频数据发送到音频输出设备，例如扬声器。
        - **音频同步：** 在播放过程中，确保音频和视频的同步是关键的。这可能需要对音频数据进行缓冲管理，以协调音频和视频的播放速度。

     综合而言，播放阶段依赖于操作系统或多媒体框架来管理图像和音频的输出。在现代计算机系统中，通常会使用专门的 API（如DirectX或OpenGL用于图形，以及WASAPI、Core Audio或ALSA用于音频）来处理这些任务。硬件加速也可以利用现代硬件的图形处理单元（GPU）和音频处理单元（DSP）来提高播放性能。这样的设计确保了音视频播放的高效性和兼容性。







## 科普小知识

### 视频如何通过视频编码再到视频格式封装的

视频编码是**将原始视频信号转换成数字信号**，并**通过压缩算法来减少数据量**，以便于存储和传输。常见的视频编码标准包括H.264、H.265、MPEG-2等。

在视频编码完成后，需要**将编码后的视频信号打包成特定的格式**，即视频封装格式。视频封装格式通常包括了**音频、视频、字幕、章节**等多个流的封装，以便于**在播放时同时进行多个流的同步播放**。常见的视频封装格式包括AVI、MKV、MP4、MOV等。

在视频编码之前，需要对视频进行采集和处理。视频采集通常是通过摄像头或其他视频设备将原始视频信号转换成数字信号。视频处理则包括对视频进行**去噪、颜色校正、降低码率**等操作，以便于视频编码器对视频进行压缩。

总的来说，视频编码和视频格式封装是视频处理和传输的重要组成部分。视频编码负责将原始视频信号压缩为较小的数据量，而视频格式封装则将编码后的视频流打包为特定的格式，以便于存储和传输。



### 一般进行录像生成的视频，也是上述流程吗

是的，一般进行录像生成的视频也会经历类似的流程。在录制视频时，摄像机或手机摄像头等设备会对视频进行采集和压缩编码，得到视频的编码数据。这些编码数据需要根据特定的封装格式进行打包，包含视频的音频、字幕、章节信息等元数据，形成最终的视频文件。不同的设备和应用程序可能会使用不同的视频编码器和封装格式，但是视频的基本处理流程是相似的。



### m3u8到底是什么格式

#### **视频播放的过程**

现在的视频网站采用的是流媒体传输协议，就是将一段视频切成无数个小段，这几个小段就是ts格式的视频文件，一段一段的网站上播放。

这样做的好处是观看更加流畅，因为他会根据网络状况自动切换视频的清晰度，在网络状况不稳定的情况下，对保障流畅播放非常有帮助。

我们可以了解下，一个视频播放的全过程。

> 1.服务器采集编码传输视频到切片器
> 2.切片器对视频创建**索引文件，**并且**切割**成**n个ts文件**
> 3.这2个文件传输到http服务器上
> 4.网站/客户端根据**索引文件**查找http服务器上的**ts文件**，连续播放这n个ts文件，就可以了。

给大家画了下流程图



![img](https://pic4.zhimg.com/80/v2-53bdb5ade54140de034973bc1bb0a38f_1440w.webp)



所以我们可以知道，索引文件非常重要，索引文件里面存储着ts文件的网络url链接，网站需要拿到索引文件，去按照url链接下载在http服务器中的ts文件，类似于爬虫。

拿到了ts文件之后，本身这些ts文件就是原视频中的一小段视频，所有ts文件下载顺序播放，就完成了整个视频的播放。

而索引文件就是m3u8文件。

现在大部分视频网站传输都是采用这种方法，所以，也就是说，如果你在观看网页视频的时候，能够弄到加载该视频的m3u8文件，那么再配合一些工具，就能下载该视频了。

该工具的作用就类似于视频网站,能够根据索引文件去下载ts文件。

更具体的耍耍m3u8文件，可参考 [【全网最全】m3u8到底是什么格式？一篇文章搞定m3u8下载](https://zhuanlan.zhihu.com/p/346683119)





#### 为什么有无损音频格式而没有无损视频压缩格式

刻意追求无损就是不讲科学的人为了求个心理安慰。

立论：视频和音频都是人类欺骗自己感官的一种手段。

一段五秒钟的视频或者音频，有可能占多大的存储空间？

答案是要多大有多大，因为这是由你的**采样质量**和**采样频率**共同决定的。

对声音和动画，不存在无损的保留。

更详细可见：[为什么有无损音频格式而没有无损视频压缩格式？](https://www.zhihu.com/question/27889429)





### 码流

码流（Bitstream）指的是一段**二进制数据流**，它包含了被编码后的数字信号，例如视频、音频、图像等等。在数字通信、数字媒体、计算机网络和信息安全等领域中，码流是一个很重要的概念。

在视频和音频领域，码流通常是通过视频编码和音频编码技术将视频和音频信号压缩成数字信号的一种形式。码流中的每一个二进制数据都代表着一些具体的信息，例如视频中的像素信息、色彩信息、帧率、码率等等，音频中的采样率、采样位数、声道数、码率等等。码流的数据量一般是通过码率来描述的，单位通常是每秒的比特数（bps）或千比特数（Kbps）。

码流的应用非常广泛，例如视频传输、视频会议、流媒体、网络广播、数字电视等等，**码流的质量和稳定性对于实时传输和播放的效果有很大的影响**。因此，在处理和传输码流时，需要注意信号的带宽、延迟、抖动等问题，以保证码流的正确性和稳定性。





#### 码流和码率的区别

码流和码率在某些情况下可以视为相同的概念，但它们**并不完全等同**。

简单来说，码流（Bitstream）是指一段连续的二进制数据流，包含了被编码后的数字信号，例如音频、视频、图像等等。而码率（Bitrate）则是指每秒钟传输的比特数，也可以理解为**码流的传输速率**。码率是码流传输中一个非常重要的指标，通常用于描述一个视频或音频流的传输速率和质量。

例如，一部高清视频文件的码流是20Mbps，这个码流中包含了视频和音频信号的所有信息。如果该视频的播放时间为1小时，则其对应的码率为：

20Mbps / 8 bits/byte = 2.5MB/s 
2.5MB/s * 60s/min * 60min/hour = 9GB/hour

这意味着，该视频在播放期间每小时会传输9GB的数据量，这个码率越高，代表传输速率越快，同时也意味着需要更大的带宽和存储空间来传输和保存这个码流。因此，在实际应用中，需要根据具体的场景和要求来控制码率的大小，以达到最优的传输效果。





### 视频压缩比

视频压缩比（Compression Ratio）是指**在压缩前后视频文件的大小比例**。具体来说，它是指压缩前的视频文件大小与压缩后文件大小的比值，通常用百分比表示。

例如，如果一个视频文件的原始大小为500MB，经过压缩后，文件大小缩小到了100MB，则该视频的压缩比为：

**压缩比 = 原始文件大小 / 压缩后文件大小** = 500MB / 100MB = 5:1

因此，该视频的压缩比为5:1，也就是说，压缩后的文件大小只有原始文件大小的五分之一。在实际应用中，视频压缩比通常越高，代表压缩效果越好，同时也意味着压缩后的文件大小更小，传输和存储成本更低。

然而，需要注意的是，视频压缩比并不是越大越好或越小越好。如果压缩比太高，会导致视频质量严重下降，出现模糊、失真、花屏等问题，影响观感和应用效果。因此，在实际应用中，需要根据具体的场景和需求来平衡视频压缩比和视频质量，以达到最优的压缩效果。





### 多码流技术

多码流技术指的是在视频压缩编码的过程中，**同时产生多个不同码率、分辨率或编码质量的视频流**。这些流可以在不同的网络环境下进行自适应码流调节，以保证视频在不同的网络条件下的播放质量和稳定性。这一技术被广泛应用于网络直播、视频点播等多媒体传输领域。

在多码流技术中，主要有以下几种类型的码流：

1. 分辨率码流：以不同的分辨率为区分，如标清、高清等。
2. 码率码流：以不同的码率为区分，常用于网络直播中。低码率适用于网络条件较差的情况，而高码率则适用于网络条件较好的情况。
3. 画质码流：以不同的画质为区分，常用于视频点播和网络直播中。通过在不同的画质和帧率之间自适应调节，可以在不影响视频质量的情况下提高视频播放效率。

通过多码流技术，视频传输可以更好地适应不同的网络环境，保证视频播放的质量和稳定性，提高用户的观看体验。
