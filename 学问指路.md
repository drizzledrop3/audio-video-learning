# 学问指路



> 入门到入土过程中所学所问所答
>
>
> 入门所看书籍有：
>
> ​	《FFmpeg入门详解 音视频原理及应用》
>
> ​	《FFmpeg从入门到精通》



## 视频如何通过视频编码再到视频格式封装的

视频编码是**将原始视频信号转换成数字信号**，并**通过压缩算法来减少数据量**，以便于存储和传输。常见的视频编码标准包括H.264、H.265、MPEG-2等。

在视频编码完成后，需要**将编码后的视频信号打包成特定的格式**，即视频封装格式。视频封装格式通常包括了**音频、视频、字幕、章节**等多个流的封装，以便于**在播放时同时进行多个流的同步播放**。常见的视频封装格式包括AVI、MKV、MP4、MOV等。

在视频编码之前，需要对视频进行采集和处理。视频采集通常是通过摄像头或其他视频设备将原始视频信号转换成数字信号。视频处理则包括对视频进行**去噪、颜色校正、降低码率**等操作，以便于视频编码器对视频进行压缩。

总的来说，视频编码和视频格式封装是视频处理和传输的重要组成部分。视频编码负责将原始视频信号压缩为较小的数据量，而视频格式封装则将编码后的视频流打包为特定的格式，以便于存储和传输。



## 一般进行录像生成的视频，也是上述流程吗

是的，一般进行录像生成的视频也会经历类似的流程。在录制视频时，摄像机或手机摄像头等设备会对视频进行采集和压缩编码，得到视频的编码数据。这些编码数据需要根据特定的封装格式进行打包，包含视频的音频、字幕、章节信息等元数据，形成最终的视频文件。不同的设备和应用程序可能会使用不同的视频编码器和封装格式，但是视频的基本处理流程是相似的。



## m3u8到底是什么格式

### **视频播放的过程**

现在的视频网站采用的是流媒体传输协议，就是将一段视频切成无数个小段，这几个小段就是ts格式的视频文件，一段一段的网站上播放。

这样做的好处是观看更加流畅，因为他会根据网络状况自动切换视频的清晰度，在网络状况不稳定的情况下，对保障流畅播放非常有帮助。

我们可以了解下，一个视频播放的全过程。

> 1.服务器采集编码传输视频到切片器
> 2.切片器对视频创建**索引文件，**并且**切割**成**n个ts文件**
> 3.这2个文件传输到http服务器上
> 4.网站/客户端根据**索引文件**查找http服务器上的**ts文件**，连续播放这n个ts文件，就可以了。

给大家画了下流程图



![img](https://pic4.zhimg.com/80/v2-53bdb5ade54140de034973bc1bb0a38f_1440w.webp)



所以我们可以知道，索引文件非常重要，索引文件里面存储着ts文件的网络url链接，网站需要拿到索引文件，去按照url链接下载在http服务器中的ts文件，类似于爬虫。

拿到了ts文件之后，本身这些ts文件就是原视频中的一小段视频，所有ts文件下载顺序播放，就完成了整个视频的播放。

而索引文件就是m3u8文件。

现在大部分视频网站传输都是采用这种方法，所以，也就是说，如果你在观看网页视频的时候，能够弄到加载该视频的m3u8文件，那么再配合一些工具，就能下载该视频了。

该工具的作用就类似于视频网站,能够根据索引文件去下载ts文件。

更具体的耍耍m3u8文件，可参考 [【全网最全】m3u8到底是什么格式？一篇文章搞定m3u8下载](https://zhuanlan.zhihu.com/p/346683119)





### 为什么有无损音频格式而没有无损视频压缩格式

刻意追求无损就是不讲科学的人为了求个心理安慰。

立论：视频和音频都是人类欺骗自己感官的一种手段。

一段五秒钟的视频或者音频，有可能占多大的存储空间？

答案是要多大有多大，因为这是由你的**采样质量**和**采样频率**共同决定的。

对声音和动画，不存在无损的保留。

更详细可见：[为什么有无损音频格式而没有无损视频压缩格式？](https://www.zhihu.com/question/27889429)





## 码流

码流（Bitstream）指的是一段**二进制数据流**，它包含了被编码后的数字信号，例如视频、音频、图像等等。在数字通信、数字媒体、计算机网络和信息安全等领域中，码流是一个很重要的概念。

在视频和音频领域，码流通常是通过视频编码和音频编码技术将视频和音频信号压缩成数字信号的一种形式。码流中的每一个二进制数据都代表着一些具体的信息，例如视频中的像素信息、色彩信息、帧率、码率等等，音频中的采样率、采样位数、声道数、码率等等。码流的数据量一般是通过码率来描述的，单位通常是每秒的比特数（bps）或千比特数（Kbps）。

码流的应用非常广泛，例如视频传输、视频会议、流媒体、网络广播、数字电视等等，**码流的质量和稳定性对于实时传输和播放的效果有很大的影响**。因此，在处理和传输码流时，需要注意信号的带宽、延迟、抖动等问题，以保证码流的正确性和稳定性。





### 码流和码率的区别

码流和码率在某些情况下可以视为相同的概念，但它们**并不完全等同**。

简单来说，码流（Bitstream）是指一段连续的二进制数据流，包含了被编码后的数字信号，例如音频、视频、图像等等。而码率（Bitrate）则是指每秒钟传输的比特数，也可以理解为**码流的传输速率**。码率是码流传输中一个非常重要的指标，通常用于描述一个视频或音频流的传输速率和质量。

例如，一部高清视频文件的码流是20Mbps，这个码流中包含了视频和音频信号的所有信息。如果该视频的播放时间为1小时，则其对应的码率为：

20Mbps / 8 bits/byte = 2.5MB/s 
2.5MB/s * 60s/min * 60min/hour = 9GB/hour

这意味着，该视频在播放期间每小时会传输9GB的数据量，这个码率越高，代表传输速率越快，同时也意味着需要更大的带宽和存储空间来传输和保存这个码流。因此，在实际应用中，需要根据具体的场景和要求来控制码率的大小，以达到最优的传输效果。





## 视频压缩比

视频压缩比（Compression Ratio）是指**在压缩前后视频文件的大小比例**。具体来说，它是指压缩前的视频文件大小与压缩后文件大小的比值，通常用百分比表示。

例如，如果一个视频文件的原始大小为500MB，经过压缩后，文件大小缩小到了100MB，则该视频的压缩比为：

**压缩比 = 原始文件大小 / 压缩后文件大小** = 500MB / 100MB = 5:1

因此，该视频的压缩比为5:1，也就是说，压缩后的文件大小只有原始文件大小的五分之一。在实际应用中，视频压缩比通常越高，代表压缩效果越好，同时也意味着压缩后的文件大小更小，传输和存储成本更低。

然而，需要注意的是，视频压缩比并不是越大越好或越小越好。如果压缩比太高，会导致视频质量严重下降，出现模糊、失真、花屏等问题，影响观感和应用效果。因此，在实际应用中，需要根据具体的场景和需求来平衡视频压缩比和视频质量，以达到最优的压缩效果。





## 多码流技术

多码流技术指的是在视频压缩编码的过程中，**同时产生多个不同码率、分辨率或编码质量的视频流**。这些流可以在不同的网络环境下进行自适应码流调节，以保证视频在不同的网络条件下的播放质量和稳定性。这一技术被广泛应用于网络直播、视频点播等多媒体传输领域。

在多码流技术中，主要有以下几种类型的码流：

1. 分辨率码流：以不同的分辨率为区分，如标清、高清等。
2. 码率码流：以不同的码率为区分，常用于网络直播中。低码率适用于网络条件较差的情况，而高码率则适用于网络条件较好的情况。
3. 画质码流：以不同的画质为区分，常用于视频点播和网络直播中。通过在不同的画质和帧率之间自适应调节，可以在不影响视频质量的情况下提高视频播放效率。

通过多码流技术，视频传输可以更好地适应不同的网络环境，保证视频播放的质量和稳定性，提高用户的观看体验。





## AVC

AVC是Advanced Video Coding（**高级视频编码**）的缩写，也称为**H.264或MPEG-4 Part 10**。它是一种基于帧内预测和运动估计的视频编码标准，可以提供更高的视频压缩比和更好的视频质量。AVC广泛应用于数字电视、高清蓝光光盘、网络视频流等领域，是当前应用最为广泛的视频编码标准之一。

AVC采用了许多先进的编码技术，如整数变换、运动补偿、熵编码等，可以有效地压缩视频数据，并保持较高的视频质量。同时，AVC还支持多码流、自适应编码、场景自适应等技术，可以根据不同的网络条件和设备特性进行编码调整，提高视频传输的效率和稳定性。

AVC的主要优点包括高效的视频压缩比、良好的视频质量、广泛的应用支持和良好的可扩展性。它在视频传输领域的应用非常广泛，如视频会议、远程监控、视频点播、网络直播等。





## Demux

在多媒体领域中，Demux（Demultiplexing）是指将一个**复合的音视频流分离成各自独立的音频流和视频流**的过程。复合的音视频流通常是由多个音频、视频和其他数据流（如字幕、菜单等）混合在一起构成的。Demux过程将这些流分离出来，以便进一步进行处理，如解码、播放、转码等。

Demux过程通常包括以下步骤：

1. 解析复合的音视频流，确定各个流的类型和位置信息；
2. 提取出音频、视频等各个流的数据；
3. 将提取出的数据送往相应的解码器进行解码；
4. 在解码后，将解码后的音频、视频等数据进行处理或直接播放。

Demux在多媒体处理中非常重要，它能够为后续的处理提供准确的音视频数据，同时也可以提高多媒体的可扩展性和可定制性，为多媒体应用提供更多的选择和灵活性。





## PCM

PCM（Pulse-Code Modulation，脉冲编码调制）是一种**模拟信号转换为数字信号**的方式。在PCM中，模拟信号在一定采样率下进行采样，每个采样值用一个固定的位数来表示，并存储为二进制数，从而转换为数字信号。PCM是一种非常常见的数字音频格式，它广泛用于音频采集、存储、传输和处理领域。

在PCM中，每个采样值通常使用8位、16位、24位或32位二进制数来表示，称为采样位数。采样率是指每秒钟采样的次数，通常采用8kHz、16kHz、44.1kHz、48kHz等标准采样率。通常，**采样位数越高、采样率越高，音频质量就越好，但是文件大小也就越大**。

在计算机领域，PCM通常指的是**无损音频编码格式**，如WAV、AIFF等。这些格式存储的音频数据是**未经压缩的PCM数据(FLAC是在WAV基础上做压缩的，但可逆，也是无损编码格式的一种)**，通常采用16位的采样位数和44.1kHz或48kHz的采样率。此外，在音频编码和解码过程中，也常常使用PCM格式来表示音频数据，如将音频数据从MP3、AAC等有损编码格式解码为PCM格式，或者将PCM格式编码为其他音频格式。





## WAV

WAV（Waveform Audio File Format）是一种常用的音频文件格式，通常用于存储音频文件。它是由Microsoft和IBM公司共同开发的，可以**存储原始音频数据**，同时支持多种音频编解码器，例如PCM、ADPCM、MP3等。WAV文件通常以.wav作为文件扩展名。由于WAV文件的**音频格式未经过压缩**，所以在音质方面不会出现失真的情况，所以WAV文件通常具有较高的音频质量，但也会导致文件大小较大。



### WAV、ACC和MP3

WAV和AAC都是音频文件格式，但它们有很多不同点：

1. 压缩方式：WAV是无损压缩的音频格式，即压缩后的音频质量和原始音频质量相同，而AAC是有损压缩的音频格式，即压缩后的音频质量会有所损失。
2. 文件大小：由于WAV是无损压缩的格式，因此WAV文件通常较大，占用存储空间较多；而AAC是有损压缩的格式，因此文件通常较小，占用存储空间较少。
3. 支持设备：由于AAC格式的压缩率高，因此它比WAV格式更适合在移动设备上播放，而WAV文件则通常用于PC或其他存储容量较大的设备上。
4. 音频质量：WAV是无损格式，因此通常具有更高的音频质量，而AAC是有损格式，因此音频质量可能会受到一定的损失。

总的来说，WAV适用于对音频质量要求高、存储容量较大的场景，而AAC适用于存储容量较小、对音频质量要求不是特别高的场景。



MP3和AAC**都是有损压缩音频格式**，它们的主要区别在于压缩算法和音质表现。

MP3是一种使用MPEG-1或MPEG-2压缩算法的音频格式，是**最早的流行的音频格式之一**。它的压缩算法基于一种称为“视听模型”的原理，即通过删除人耳无法察觉的音频信号来减小文件大小。虽然MP3文件的音质相对于原始音频有所损失，但是由于它的普及和广泛应用，很多人已经适应了它的音质。

AAC是一种更先进的音频格式，它采用了**更高效的压缩算法**，可以在相同的码率下提供更好的音质。**与MP3相比，AAC具有更好的音频还原性和更小的文件大小**。AAC的压缩算法基于一种称为“感知编码”或“psychoacoustic modeling”的原理，它能够更好地抑制人耳无法察觉的信号，以提高压缩比并减小文件大小。AAC还支持多通道音频，因此在处理复杂的音频场景（例如电影和游戏）时更加灵活。

总的来说，AAC在音质和文件大小方面比MP3表现更好，尤其在相同的码率下。但由于MP3是历史上最早和最广泛使用的音频格式之一，因此在一些设备和软件上，MP3仍然是主要的音频格式之一。





## AIFF

AIFF是一种音频文件格式，全称为Audio Interchange File Format，它是由苹果公司开发的一种**无损音频格式**。与WAV格式类似，AIFF文件也是一种容器格式，可以容纳多种编码方式的音频数据，其中最常用的编码方式是PCM编码。

与WAV格式不同的是，AIFF格式对于音频数据的存储方式采用了Big-Endian的字节序。另外，AIFF文件头的格式也略有不同，可以存储一些额外的元数据信息。但是总的来说，AIFF和WAV这两种格式在数据内容上是非常相似的。



## 声音三要素

声音的三要素分别是频率、振幅和波形。

- **频率**：声音的频率指的是声波振动的快慢，也就是声音的高低**音调**。它用赫兹（Hz）表示，即每秒振动次数。例如，人类可听到的频率范围为20 Hz到20,000 Hz，而中央C的频率是261.63 Hz。
- **振幅**：声音的振幅指声波振动的幅度大小，也就是声音的**音量**强弱。它通常用分贝（dB）表示，是声音压力级的对数比例。例如，60 dB的声音比30 dB的声音强2倍。
- **波形**：声音的波形指的是声波振动的形态，也就是声音的**音色**。它通常用波形图表示，是声波振动的形状。例如，正弦波形是最简单的声波形态，而各种复杂的波形则代表着不同的音色。

举例来说，如果有一段音频的频率为440 Hz（即A4音符的频率）、振幅为60 dB，波形为正弦波形，那么这段音频就会产生一个高音调、强度适中、纯净的声音。



### 为什么音调可以指频率

在听到一个声音时，耳朵会感知到声波震动的频率，这个频率会被传递到**大脑进行处理，形成对应的音调感知**。因此，通常我们将音调与频率等同起来。举个例子，当听到一个声音的频率为440Hz时，人们通常会感知到它的音调是中央C。



### 声音的强度和响度

声音的强度和响度是与频率、振幅和波形都有关系的概念。

声音的**强度指的是声波传播过程中携带的能量大小**，它与声音的**振幅相关**。振幅越大，声波携带的能量就越多，声音的强度也就越大。举个例子，一个人大声喊叫，声音的强度就比平时说话时要大。

而**响度则是指人耳对声音强度的<u>主观感受</u>**，是一个相对概念，其单位为分贝（dB）。相同的声音强度在不同频率下，人耳感受到的响度也会不同。一般来说，当声音强度增加10倍时，人耳感受到的响度会增加约3分贝。举个例子，听音乐时，如果音量太大，人们可能会感到噪音刺耳，这时可以降低音量来减小响度。





## Nyquist

Nyquist是指数字信号处理中的一个重要理论，也被称为**奈奎斯特定理或奈氏采样定理**。该定理表明，**一个信号的采样频率必须至少是信号中<u>最高频率成分的两倍</u>才能对原始信号进行准确的重建，否则会产生混叠误差，导致信号失真。**因此，Nyquist理论对于数字信号处理和音频、视频等数字媒体领域都非常重要。

举个例子，比如说人耳可以听到的最高频率约为20kHz，那么如果要对一个音频信号进行数字化处理，采样频率至少应该是40kHz才能准确地重建该信号。如果采样频率不够高，可能会发生混叠现象，即原来的高频信号被混叠到了低频信号中，从而影响信号的质量。



## 声音的模数转换

在数字声音中，声音的模拟信号需要经过三个过程进行模数转换，包括**采样、量化和编码**。

首先是采样过程，即**将连续的模拟信号离散化**，将其转换成一系列离散的采样值。这个过程是**由采样率控制**的，采样率越高，离散化程度越小，能够更好地保留原始信号的细节。举个例子，如果我们使用CD标准的采样率44.1kHz进行采样，每秒钟将会得到44100个采样点。

接下来是量化过程，即**将采样值的幅度进行数字化处理**，将其**转换成离散的数字表示**。量化过程中，采样值的幅度被分为一定的等级，每个等级对应一个数字，以此来近似模拟信号的幅度。量化的过程会造成一定的信息损失，因为每个采样值都被近似为一个数字。量化的精度由量化位数来决定，**位数越多，精度越高**，但所需的数据空间也越大。以16位量化位数为例，每个采样点的值都可以用一个16位的二进制数来表示，其中最高位为符号位。

最后是编码过程，即将**采样值的数字表示转换成二进制码流**，以便于数字存储和传输。编码的方式有很多种，例如Pulse Code Modulation (PCM)编码、Delta Modulation (DM)编码等。PCM编码是一种常用的编码方式，它将采样值用二进制数表示，并将其排列成字节流的形式。

总的来说，采样、量化和编码三个过程是数字声音模数转换的核心，它们将模拟声音转换成数字信号，以便于数字存储、传输和处理。





## 音轨

音轨是指**一段音频数据**，在视频文件中通常指的是视频文件中的**一条音频流**，用于存储影片中的音频信息。在音频文件中，一般指的是一条音频流或一个音频轨道，用于存储音频数据。音轨可以包含多种不同的编码方式和格式，比如常见的MP3、AAC、WAV等音频格式。在音视频处理中，音轨是非常重要的概念之一，通过对音轨的提取、分离、混合、转码等操作可以实现对音频数据的处理。



### 音视频中分离出各个音轨

对于音频来说，通常情况下一个音频文件只包含一个音轨，但是对于多个音轨的情况（如多语言版本、不同的音频格式等），可以**通过解复用（demux）操作分离出不同的音轨**。一些音频处理工具，如FFmpeg、Audacity等，提供了音频解复用的功能。

对于视频来说，一个视频文件通常包含一个或多个视频轨道以及一个或多个音频轨道。视频解复用可以将视频文件中的视频轨道和音频轨道分别提取出来。FFmpeg是一个功能强大的视频处理工具，它可以用来解复用、剪辑、转码、压缩等多种操作，对于从音视频中分离出音轨，可以使用以下命令：

```cmd
# 提取视频中所有音轨，输出为单独的音频文件
ffmpeg -i input.mp4 -vn -acodec copy audio_track_%d.aac

# 提取视频中的第 N 个音轨，输出为单独的音频文件
ffmpeg -i input.mp4 -map 0:a:N -acodec copy audio_track_N.aac
```

其中，`-vn`选项表示不提取视频轨道，`-acodec copy`选项表示音频编码格式不变，直接拷贝到输出文件中。`-map`选项可以指定输入文件中要提取的音轨，`N`表示音轨序号，从0开始编号。





## 声卡

声卡是**计算机硬件系统中的一种重要组成部分**，主要用于**音频信号的输入、输出、放大、处理和转换**等功能。它是计算机和音频设备之间的一个接口，可以使计算机与外部音频设备（如扬声器、麦克风、音响等）进行数据交换，将**数字信号转换成声音信号或将声音信号转换成数字信号**。

以PC机上的声卡为例，它可以将声音信号从计算机中的音频软件（如媒体播放器、音频编辑器等）发送到扬声器或耳机中，同时可以从麦克风或其他音频设备中接收声音信号，将其转换成数字信号后传输给计算机进行处理。声卡还可以实现音频效果的调整、降噪等处理，提高音质和信噪比。

总之，声卡是音频处理中不可或缺的一部分，它可以帮助我们实现各种音频输入输出的操作，为我们的音乐、影视等多媒体应用提供良好的音质和音效。





## WebRTC

WebRTC（Web Real-Time Communication）是一种实时通信技术，使浏览器和移动应用程序能够实现音频、视频和数据共享，而无需安装任何插件或额外的软件。

WebRTC 最初由 Google 开发，现在已成为 W3C 的标准之一。它提供了一种使用 JavaScript API 来构建音频和视频聊天的方法，它还支持文件共享和屏幕共享等功能。

WebRTC 技术可以在现代浏览器中直接使用，例如 Chrome，Firefox 和 Safari。 它的主要应用领域包括在线客服、在线会议、远程协作、网络游戏等。





## 回声消除

回声消除（Echo Cancellation）技术是指**消除语音通信中由于麦克风和扬声器的相互作用产生的回声**的一种信号处理技术。在语音通信过程中，由于麦克风和扬声器的声音存在时间上的延迟，会导致麦克风捕捉到自己发出的声音，进而形成回声。回声会降低通信质量，影响用户体验。

回声消除技术通过使用算法将已知的音频信号和待处理的音频信号进行比较，然后消除回声信号。它通常基于自适应滤波器，该滤波器可以将已知信号中的回声部分估算出来，并将其从待处理信号中减去，从而达到回声消除的效果。





## 舒适噪声产生技术

舒适噪声产生技术是指通过特定的算法和信号处理技术，在室内或者车内等封闭空间中，通过**发出一定频率和振幅的噪声，来<u>掩盖环境中不舒适的噪声</u>**，使人们感觉到更加舒适和愉悦。

在室内或车内等封闭空间中，通常会出现机器运转声、空调噪音、交通噪声等不舒适的噪声，这些噪声会引起人们的不适，甚至会影响人们的工作和生活。舒适噪声产生技术通过产生特定的噪声信号，将不舒适的噪声掩盖或者减弱，从而降低不适感，提升人们的生活质量和工作效率。

此外，舒适噪声产生技术还可以应用在音乐播放器、助眠设备等产品中，通过发出一定频率和振幅的噪声，来创造出一种舒适、放松的氛围，帮助人们放松身心，促进睡眠和休息。





## 混音技术

混音技术指的是将多个音频信号混合在一起，产生出一个单一的音频输出的过程。混音通常包括以下步骤：

1. 输入信号选择：从多个音频信号中选择要混合的信号。
2. 均衡：调整各输入信号的频率响应，以确保它们在混音后的音量均衡。
3. 平衡：调整各输入信号的音量，以确保它们在混音后的音量平衡。
4. 音量处理：对输入信号进行音量增益、压缩、限制等处理，以确保它们在混音后的音量达到预期的水平。
5. 空间处理：对输入信号进行混响、延迟、立体声处理等，以改善音频的空间感。
6. 输出处理：将混合后的信号输出到音频设备或文件中。

混音技术广泛应用于音乐制作、电影制作、广播电视等领域。在音乐制作中，混音是制作出一首完整的歌曲的重要步骤，通过混音可以将各个乐器和声音混合在一起，产生出一个完整的音乐作品。在电影制作中，混音是将不同音频轨道混合在一起，产生出一个整体音效的重要步骤。在广播电视中，混音是将不同音频信号混合在一起，产生出一个完整的节目音效的重要步骤。





## 音频编码流程

音频编码是将音频信号转换成数字信号的过程，通常包括以下几种基本流程：

1. 采样：将模拟音频信号转换成数字信号的过程。采样率表示每秒钟采样的次数，常见的采样率有44.1kHz、48kHz等。
   举例说明：以CD音质为例，CD音质的采样率为44.1kHz，也就是每秒钟会对音频信号进行44100次采样。
2. 量化：将采样后的音频信号进行量化，将采样值量化成一个个离散的数字，采用多少位来表示采样值就决定了量化的精度。
   举例说明：比如CD音质使用16位来表示采样值，就可以表示2^16=65536个不同的离散数值。
3. 压缩：通过去除信号中一些冗余信息，将数字信号的数据量减少，从而降低存储和传输的成本。
   举例说明：目前常见的音频压缩格式有MP3、AAC、FLAC等。
4. 编码：将采样和量化后的数字信号进行编码，以便在存储和传输过程中能够被有效地处理和解码。
   举例说明：常见的音频编码算法有PCM编码、ADPCM编码、MP3编码、AAC编码等。

这些基本手段可以组合使用，形成不同的音频编码格式和算法，以满足不同的需求。





## 掩蔽效应

在音频信号处理中，掩蔽效应是指**某个频率分量的能量被较高能量的邻域频率分量掩盖，使得该频率分量对人耳不可感知**。掩蔽效应分两类，**同时掩蔽效应和短时掩蔽效应**，下面进行详细解释：

1. 同时掩蔽效应

同时掩蔽效应是指，当一个频率分量的能量受到邻域频率分量的掩盖时，只有在这些邻域频率分量的能量超过该频率分量的能量一定比例时，该频率分量才会被掩盖，否则该频率分量仍然可以被人耳感知。

同时掩蔽效应还与频率分量的相对位置有关，一般来说，相邻频率分量之间的同时掩蔽效应比较强，而远离的频率分量之间的同时掩蔽效应则比较弱。

2. 短时掩蔽效应

短时掩蔽效应是指，当一个频率分量的能量突然增加或减少时，**该频率分量周围一段时间内的邻域频率分量可能会被掩盖**，直到该频率分量的能量恢复到稳定状态后，邻域频率分量才能再次被人耳感知。

短时掩蔽效应主要发生在音频信号的快速变化和瞬时峰值处，这种效应对音频信号的编码和解码有着很重要的影响，通常需要在编码器和解码器中进行一定的处理，以保证编码后的音频质量尽可能接近原始音频。





## HSV/HSI

HSV全称是Hue, Saturation, Value的缩写，也叫作色相/色调、饱和度和亮度。它是一种颜色空间，通常用于颜色的计算和调整。Hue表示色相，Saturation表示饱和度，Value表示亮度。

在HSV颜色空间中，Hue的取值范围为0-360度，表示了颜色的种类，如红、绿、蓝等；Saturation的取值范围为0-1，表示颜色的饱和度，饱和度高的颜色看起来更加鲜艳；Value的取值范围也为0-1，表示颜色的亮度，亮度高的颜色看起来更加明亮。

**色相/色调**（Hue）是指颜色的品种，也就是人们通常所说的**颜色名称**，如红色、黄色、蓝色等。色相是由光谱颜色波长决定的，它是一个色彩分类的基本维度。
例如，红色、黄色、绿色、蓝色等颜色就是不同的色相。在HSV颜色空间中，色相值的范围为0到360度，0度表示红色，60度表示黄色，120度表示绿色，180度表示青色，240度表示蓝色，300度表示品红色，360度又表示红色。因此，如果一个像素在HSV空间的色相值为0度，则该像素的颜色为红色。



**饱和度**（Saturation）是指颜色的纯度或强度，表示**颜色的鲜艳程度**。在HSV颜色空间中，饱和度取值范围为0到1，0表示该颜色为灰色，1表示该颜色为纯色。
举个例子，假设有一张图中有一片绿色的草坪，如果将这张图的饱和度调低，那么草坪的颜色就会变得暗淡，不再像原来那样鲜艳。反之，如果将饱和度调高，那么草坪的颜色就会变得更加饱满，更加鲜艳。



**亮度**（Value，V / Intensity,I）是指颜色的明暗程度。在HSV颜色空间中，亮度指的是颜色的明度，表示**颜色的明亮程度**，它的值从0到1，0代表黑色，1代表白色。例如，RGB(255, 255, 255)在HSV颜色空间中的亮度值为1，而RGB(0, 0, 0)在HSV颜色空间中的亮度值为0。
举个例子，在一张图片中，一朵花有不同的颜色，有些花的颜色比较淡，有些花的颜色比较鲜艳，如果我们想要把花朵的颜色分离出来，可以使用HSV颜色空间中的饱和度和亮度属性，对图像进行处理，得到不同饱和度和亮度的花朵区域。



颜色的鲜艳程度指的是颜色的饱和度，也就是颜色的强度和纯度，而明亮程度指的是颜色的亮度或明暗程度。

举个例子，红色和粉色都是属于红色色相，但红色更鲜艳，也就是饱和度更高，而**粉色则更柔和，饱和度相对较低**。另外，红色可以是比较亮的红色或比较暗的红色，这就涉及到明亮程度的问题。例如，**艳红色可以是亮红色，也可以是暗红色**。

总的来说，颜色的鲜艳程度和明亮程度都是影响颜色感知的重要因素，但它们所表现的方面是不同的。

由于色相/色调直观表现颜色种类、饱和度直观体现颜色中灰度比例（也即颜色鲜艳程度）、亮度展示了颜色的明亮程度，因此HSV颜色空间相对于RGB颜色空间更容易人类理解和调整，在许多颜色处理任务中被广泛应用。





## 色度采样

色度**4:2:0是一种常见的色彩采样格式**，广泛应用于数字视频、图像和压缩领域。在色度4:2:0中，图像的亮度（Y）分量被完全保留，而色度（Cb和Cr）分量则被降低采样，以实现压缩。

具体来说，色度4:2:0采样是指对**每4个像素中的2个像素进行Cb和Cr分量的采样，而Y分量则在每个像素中都有采样**。这样一来，**色度分量的采样率被降低了一半**，从而实现了图像的压缩。

例如，对于一幅分辨率为1920×1080的图像，如果采用色度4:2:0采样，那么Cb和Cr分量的采样率将变为960×540，即分辨率被降低了一半。这样一来，图像的数据量也被大大减少，可以在一定程度上降低传输和存储的成本。

尽管色度4:2:0采样可以实现图像的压缩，但由于色度分量的采样率被降低，所以在图像中可能会出现颜色失真和锯齿状边缘等问题。因此，在进行视频编码和传输时，需要根据实际应用场景选择合适的色度采样格式，以实现较高的压缩比和较好的视觉质量。



除了色度4:2:0之外，常见的色度采样比例还包括：

1. 色度4:4:4：在每个像素中都进行Y、Cb和Cr分量的采样，即采样率都为1，可以获得最高的色彩精度和清晰度，但数据量也最大。
2. 色度4:2:2：在每2个像素中共享一个Cb和Cr分量，即Cb和Cr分量的采样率为1/2，可以实现较高的压缩比和较好的图像质量，但数据量仍然较大。
3. 色度4:1:1：在每4个像素中共享一个Cb和Cr分量，即Cb和Cr分量的采样率为1/4，可以实现更高的压缩比，但图像质量相对较低。
4. 色度4:2:0：在每4个像素中共享一个Cb和Cr分量，即Cb和Cr分量的采样率为1/2，可以实现更高的压缩比，但图像质量相对较低。

不同的色度采样比例可以根据实际需要进行选择，通常是在图像质量和数据量之间进行权衡，以满足不同应用场景的需求。

> 视频像素格式：
>
> 1. yuv420p: YUV 4:2:0是最常见的视频色彩空间格式之一。它使用8位来表示亮度（Y）分量，以及每4个像素共享一组颜色差（色度）分量（U和V）。
> 2. yuvj420p: 类似于yuv420p，但是使用完整范围（Full Range）表示亮度分量，而不是默认的广播范围。
> 3. yuv422p: YUV 4:2:2是一种色彩空间格式，相比于yuv420p，它使用更多的色度采样，即每两个像素共享一组色度分量。
> 4. yuvj422p: 类似于yuv422p，但使用完整范围表示亮度分量。
> 5. yuv444p: YUV 4:4:4是一种无损的色彩空间格式，每个像素的亮度和色度分量都是独立的，没有色度采样。
> 6. yuvj444p: 类似于yuv444p，但使用完整范围表示亮度分量。
> 7. nv12: NV12是一种平面格式，亮度分量（Y）占据一个平面，而色度分量（U和V）交错存储在另一个平面中。
> 8. nv16: 类似于nv12，但色度分量是分开存储的，而不是交错存储。
> 9. nv21: 类似于nv12，但是色度分量的顺序与nv12相反。
> 10. yuv420p10le: 类似于yuv420p，但是使用10位来表示亮度和色度分量，LE表示低字节序（Little-Endian）。
> 11. yuv422p10le: 类似于yuv422p，但使用10位表示。
> 12. yuv444p10le: 类似于yuv444p，但使用10位表示。
> 13. nv20le: 类似于nv12，但使用10位表示。
> 14. gray: 灰度图像，只有亮度分量，没有色度分量。
> 15. gray10le: 类似于gray，但使用10位表示。





## CMYK

CMYK是一种常用的颜色模式，**常用于印刷和出版领域**。CMYK是由四种颜色（**青色Cyan、洋红色Magenta、黄色Yellow和黑色Key**）组合而成的，表示了可见光谱中的颜色与印刷油墨的混合比例。在CMYK颜色模式下，图像的颜色信息由这四种颜色的百分比构成，其中的K表示黑色，因为印刷过程中黑色是最常用的一种颜色，而且印刷油墨中的黑色比其他颜色更容易控制。CMYK的颜色空间较小，因此在图像处理中需要进行颜色空间转换，例如从CMYK转换到RGB。





## IPB帧

IPB帧是视频编码中的一种帧类型，通常用于H.264/AVC和H.265/HEVC编码标准中。它是由三种类型的帧组成的序列，包括I帧（帧内编码帧）、P帧（向前预测帧）和B帧（双向预测帧）。

I帧是完整的图像帧，每个I帧都是关键帧，包含了完整的图像信息，不依赖于其他帧的信息。P帧是向前预测帧，根据前一个I帧或P帧进行预测，只保存图像差异，通常比I帧大小小。B帧是双向预测帧，同时参考前一帧和后一帧进行预测，保存更少的差异信息，通常比P帧更小。

IPB帧序列的使用可以减少视频编码的位率，并提高视频的压缩比。I帧的帧间无依赖性，使得视频快进和快退操作更加灵活，而P帧和B帧则可以有效地利用图像中的时空冗余性，提高压缩效率和视频质量。

三类帧的工作方式如下：

1. I帧（Intra-coded frame）：I帧是编码视频序列中的**关键帧**（也叫做帧同步帧），可以被解码后单独显示。I帧的编码不依赖于其他帧，而是只利用自己的信息进行编码。因此，**I帧的压缩率相对较低，但是它可以提供比较好的图像质量**。在解码过程中，如果遇到I帧，则**直接显示I帧**。
2. P帧（Predicted frame）：P帧是编码视频序列中的预测帧，编码时**依赖于前面的关键帧（I帧）和/或预测帧（P帧）**，可以看做是对前一关键帧（I帧）或前一预测帧（P帧）的**差分编码**。在解码过程中，P帧需要**先解码前面的帧，才能进行解码和显示**。因此，解码顺序与编码顺序不一致，一般需要等待前面的帧解码完成后才能进行解码和显示。
3. B帧（Bidirectional predicted frame）：B帧也是预测帧，但是与P帧不同的是，B帧**同时依赖于前面的关键帧（I帧）和后面的关键帧（I帧）**，也就是说，B帧是对前后两个关键帧的差分编码。在解码过程中，B帧需要**等待前后的帧都解码完成后，才能进行解码和显示**。因此，**解码顺序与编码顺序也不一致**。

总体来说，I帧是独立的关键帧，可以单独解码和显示；P帧和B帧是预测帧，需要依赖前后的帧进行解码和显示，因此解码顺序和编码顺序不一致，需要进行**帧缓存和重排**。



### B帧的影响

在 H.264 中，B 帧是一种双向预测帧，它可以根据前后两个帧进行预测，从而提高视频的压缩比。但是，B 帧确实存在一个问题，即**它的编码顺序和播放顺序并不一致，这就需要在解码时进行一些额外的处理**。

对于没有后续帧的 B 帧，一般会使用一些技术来减少其产生的影响。一种常用的方法是**将 B 帧重新编码成 P 帧或者 I 帧**，这样就可以避免解码时出现问题。另一种方法是**使用流控制算法，尽可能地减少产生 B 帧的情况**，从而减小其带来的影响。

在**实时传输**（比如视频会议）中，为了保证视频的流畅性和延时性，往往会采用一些特定的编码参数，例如**使用更少的 B 帧、调整帧率**等，来提高视频的实时性。





## IDR帧

IDR帧（Instantaneous Decoder Refresh frame），即**即时解码刷新帧**，是视频编码中的**一种关键帧类型**。

在视频编码过程中，通常会设置一个**GOP（Group of Pictures）组**，由一组I帧、P帧、B帧交错组成。其中I帧为关键帧，可以作为独立的帧来解码，而P帧和B帧则依赖于之前的关键帧和之前的P帧和B帧。

IDR帧和普通I帧的主要区别在于它们的作用和使用场景不同。

IDR帧（也称为关键帧或I帧）是视频编码中的一个特殊类型的帧，**通常是一组视频序列的第一帧，或者是一个隔一段时间出现的特殊关键帧**。**解码到IDR帧时会强制清空前后向参考帧列表（Decoded Picture Buffer，DPB），将已解码的数据全部输出或者抛弃，然后开始一次全新的解码序列**。IDR帧的作用是在视频解码过程中**提供一个随时可以从这一帧开始解码的起点**，使得在解码时能够避免出现错误传播和累积，同时也可以提高视频的随机访问能力，即可以在任意位置开始播放视频。IDR帧与其他类型的帧（如P帧和B帧）有不同的编码方式，通常会比其他类型的帧要大，因为它必须包含所有的视频信息。

普通的I帧也是一种关键帧，但是它不同于IDR帧，**不具备IDR帧的重要性**。在一组视频序列中，普通的I帧通常作为一个视频序列中的第一帧或某些关键帧。**普通I帧不像IDR帧那样强制要求解码器重置**，但是如果视频中的P帧或B帧丢失了，解码器需要一个I帧作为参考帧来恢复视频。

总而言之，IDR帧通常用于视频流的随机访问点，比如视频的开始位置、关键帧之间的间隔等。在H.264/AVC中，IDR帧的类型码为5。





## 帧缓存和重排

在视频编解码过程中，帧缓存和重排是两个重要的步骤。

帧缓存是指**将已解码的视频帧保存在缓存**中，以便之后使用。在视频编解码中，由于视频帧之间存在时间上的关联，因此需要对视频帧进行缓存以便后续的处理。例如，当解码一帧视频时，它可能需要参考之前的一些帧，这些帧需要被缓存下来。另外，当视频需要被显示时，也需要从帧缓存中取出相应的帧进行处理。

帧重排是指**将缓存的视频帧按照显示顺序进行排列**，以便进行显示。在视频编解码中，由于帧缓存中的帧可能并不是按照显示顺序保存的，因此需要对其进行重排。帧重排通常需要参考视频帧的时间戳等信息，将缓存中的帧按照正确的顺序进行排列，以便进行显示。

具体来说，在视频解码过程中，当解码器解码一帧视频时，会将该帧存储在帧缓存中。当视频需要进行显示时，解码器会将帧缓存中的视频帧按照正确的顺序进行重排，并将它们交给显示器进行显示。这样就可以确保视频的顺序正确，并且能够流畅地进行播放。





## 熵编码

熵编码（Entropy Coding）是一种数据压缩技术，**利用信息的统计特性对数据进行编码**，使得编码后的数据占用更少的存储空间，从而达到压缩数据的目的。

常见的熵编码算法有香农编码（Shannon Coding）、哈夫曼编码（Huffman Coding）和算术编码（Arithmetic Coding）。

香农编码是一种熵编码算法，由克劳德·香农于1948年提出，用于对符号进行无损压缩。
香农编码的基本思想是**根据符号出现的概率来为其分配可变长度的编码**，出现概率越高的符号对应的编码越短，出现概率越低的符号对应的编码越长，这样可以达到更高的压缩比。
具体实现时，需要先计算出每个符号出现的概率，然后根据概率为每个符号分配编码。一般使用二进制编码，即用0表示符号编码的左分支，用1表示右分支。编码过程是从根节点开始，根据符号的出现概率依次向下选择左右分支，直到找到对应的符号。
香农编码具有无损压缩、可逆性和唯一性等特点，是一种重要的数据压缩算法。



哈夫曼编码是一种变长编码，它**根据待编码数据的统计特性，为每个数据符号分配一个唯一的编码**，使得出现频率高的符号分配的编码短，出现频率低的符号分配的编码长，从而达到压缩数据的目的。在编码和解码的过程中，需要通过哈夫曼树来查找每个符号对应的编码，因此需要先进行一次统计分析得到哈夫曼树。哈夫曼编码广泛应用于数据压缩、图像和音频压缩等领域。



算术编码是一种无损数据压缩算法，它是一种基于统计模型的编码方式，可以有效地压缩数据。它的核心思想是将输入数据视为一个符号串，然后根据每个符号的概率分布进行编码，概率越高的符号使用越短的编码，概率越低的符号使用越长的编码。
算术编码的过程是先将符号串转化为一个实数值，然后将实数值转化为一个二进制数作为压缩后的数据。具体地，算术编码的过程可以分为两个阶段：

1. 建立符号概率模型。首先需要统计输入数据中每个符号的出现频率，并将其转化为符号概率分布模型。这个模型可以使用各种统计方法得到，如经验概率分布、自适应模型、基于上下文的模型等。
2. 进行编码。编码的过程就是将符号串转化为一个实数值。具体地，算术编码将整个符号串视为一个区间，然后每个符号根据其概率分布被映射到区间上的一个子区间。最终，整个符号串所映射到的子区间就是算术编码的结果，将该子区间的上下界转化为二进制数即可。

算术编码具有压缩率高、适用于任意类型的数据、数据解压速度慢等特点。由于算术编码过程中需要保存实数值，因此需要精确的数学计算，这就导致了算术编码解码速度较慢的问题





## 视频数据冗余

1. **空间冗余**：在空间域中，**相邻像素之间通常会有很强的相关性**，可以通过空间域滤波、亮度/色度分离等方式来减少空间冗余。
2. **时间冗余**：在时间域中，视频信号通常是一系列连续的图像帧，**相邻帧之间也有很强的相关性**，可以通过运动估计、帧内/帧间压缩等方式来减少时间冗余。
3. **编码冗余**：视频信号的表示通常采用数字信号处理中的各种编码方法，比如图像压缩中的熵编码、变长编码等，这些**编码方法通常会带来一定的冗余**，可以通过优化编码算法、采用更高效的编码方式来减少编码冗余。
4. **视觉冗余**：**人眼对于视频信号中一些细节信息的感知并不敏感**，比如高频细节、低亮度区域等，这些信息可以在编码过程中被丢弃，从而减少视觉冗余。
5. **知识冗余**：**规律性的结构可由先验知识和背景知识得到**，视频信号中通常包含一些特定的场景、物体、动作等，这些信息可以**通过知识建模、上下文分析等方式来提高压缩效率**，从而减少知识冗余。







## 编码流程

### 关键步骤

1. 预测：预测是视频编码的关键技术之一，通过利用视频信号在空间和时间上的相关性，对当前帧进行预测，从而减少冗余信息的传输，提高编码效率。
2. 变换：变换是视频编码的另一个关键技术，通过将图像从时域变换到频域，减少冗余信息的传输，提高编码效率。
3. 量化：量化是将变换后的视频信号映射到一定的量化等级上，从而减少需要传输的信息量，但也会带来一定的失真。
4. 熵编码：熵编码是通过对视频信号进行编码，进一步减少需要传输的信息量，常用的有霍夫曼编码和算术编码等。



### 一般流程

1. 分帧：将视频信号分成若干个图像帧。
2. 预处理：对每个图像帧进行去噪、锐化、色彩校正等预处理操作，提高视频质量。
3. 运动估计和补偿：对当前帧进行运动估计，并将估计结果用于下一帧的编码，从而减少冗余信息的传输。
4. 变换和量化：对运动补偿后的帧进行变换和量化操作，得到频域系数。
5. 重排：由于视频帧中每个宏块（或子块）的编码顺序与扫描顺序不同，“重排”将DCT系数从扫描顺序转换为编码顺序，从而提高压缩比并减小码流大小。
6. 熵编码：对量化后的系数进行熵编码，得到最终的编码结果。





## 运动估计和补偿

在视频编码中，为了尽可能地**减少冗余信息并提高压缩比**，采用了运动估计和补偿技术。

**运动估计是指通过对相邻视频帧的比较，找到其中的相似区域，并计算出它们之间的运动信息**，例如位移、旋转、缩放等。这些运动信息可以被用来生成一个运动矢量，表示目标区域相对于参考帧的运动情况。

运动补偿则是**通过将目标区域在<u>参考帧中的位置与运动矢量相结合，得到在当前帧中的位置</u>，进而实现对目标区域的复制**。

运动估计和补偿技术的关键是如何准确地计算运动矢量。常见的算法有全搜索法、三步搜索法、分层搜索法等。全搜索法是最为简单直接的方法，但计算量较大。三步搜索法则将搜索过程分为三个步骤，以降低计算复杂度。分层搜索法则将图像分解为不同的分辨率层次，分别进行搜索，从而进一步提高搜索效率。

运动估计和补偿技术可以很好地减少视频编码中的冗余信息，从而实现更高效的视频压缩。







## 变换和量化

视频编码中的变换和量化是压缩视频数据的两个关键步骤。**变换通过将原始图像转换为一组频率系数来利用图像的空间频率信息。量化是将这些频率系数舍入为较小的值，以减少数据量并实现压缩。**

变换是指**将视频帧中的空间域信息转换到频域（频率域）**，常用的变换方法包括离散余弦变换（DCT）、离散小波变换（DWT）等。变换可以去除视频帧中的空间冗余，即像素之间的冗余信息。在变换后的频域中，视频帧的信息可以被表示为不同频率的系数，这些系数中高频部分通常可以被量化为0，从而达到压缩的目的。

量化是通过**在变换系数上应用量化表来<u>舍入频率系数并减少其精度以减少数据量</u>**。量化表是由编码器预定义的，根据图像块中不同频率的系数级别来确定不同系数值的大小。由于量化过程引入了损失，因此需要根据不同场景和应用程序的需求来平衡图像质量和数据量。因此，在量化过程中，通过控制量化表的大小和量化步长等参数，可以实现不同的压缩效果。

在解码器端，反量化和反变换的操作被用于重构原始像素值。通过使用编码器和解码器之间相同的量化表，解码器可以通过将量化后的系数乘以相应的量化步长来进行反量化。接下来，反变换被用于将频率系数转换回空间域图像，从而重构出压缩前的图像。

在整个编码过程中，变换和量化步骤的选择和参数设置对编码效率和图像质量都有重要的影响。需要根据具体应用场景进行选择和调整。





## 重排

在视频编码中，经过变化和量化后，**编码器会将编码后的系数按照特定的顺序排列，以便于压缩和传输**。这个过程被称为重排（reordering）。

重排的过程可以通过对编码后的系数进行扫描来实现，通常有两种常见的扫描方式：逐行扫描和逐列扫描。逐行扫描是指按照行的顺序排列编码后的系数，而逐列扫描则是按照列的顺序排列编码后的系数。

重排的主要作用是提高压缩效率和编码质量。因为编码器在压缩和传输过程中通常会采用一些针对特定顺序的压缩算法，通过重排可以让编码器更好地利用这些算法，从而提高压缩效率和编码质量。

例如，在H.264/AVC中，编码器会对编码后的系数进行熵编码。如果按照特定的顺序排列系数，就可以提高熵编码的效率，从而进一步提高压缩效率和编码质量。重排还可以减少码流中的零值，进一步提高压缩效率。

通过优化系数排列顺序，可以提高压缩效率和编码质量，从而实现更好的视频压缩和传输效果。

### zigzag

Zigzag 是一种数据重排算法，常用于图像和视频编码中的**离散余弦变换（DCT）**和离散余弦逆变换（IDCT）中。

在图像和视频编码中，DCT 和 IDCT 是重要的信号变换和逆变换，用于将原始图像或视频数据从空间域转换到频域，以实现数据压缩和降低信号的冗余性。在进行 DCT 和 IDCT 变换时，通常需要将数据从二维矩阵按照一定的顺序重新排列，这就是 Zigzag 算法的作用。

Zigzag 算法的基本思想是将二维矩阵按照一定的顺序重新排列成一维数组，以便更方便地进行 DCT 或 IDCT 变换。具体来说，Zigzag 算法将矩阵中的元素按照如下顺序排列成一维数组：

```c++
0  1  5  6  14 15 27 28
2  4  7  13 16 26 29 42
3  8  12 17 25 30 41 43
9  11 18 24 31 40 44 53
10 19 23 32 39 45 52 54
20 22 33 38 46 51 55 60
21 34 37 47 50 56 59 61
35 36 48 49 57 58 62 63
```

其中第一行和第一列的元素排列顺序固定，其余元素按照如下的“**之”字形顺序排列**。

使用 Zigzag 算法可以将二维矩阵按照一定的顺序重新排列成一维数组，方便进行信号变换和逆变换，同时也有利于数据的压缩和编码。在 JPEG 图像压缩中，Zigzag 算法常被用于将 DCT 变换后的系数转换为一维数组，以便进行霍夫曼编码等压缩技术的应用。





## 压缩编码

### 预测编码

预测编码是指**利用信号的前一时刻或前几个时刻的取值来预测当前时刻的取值，然后将预测误差编码并传输或存储**。预测编码在图像和视频压缩领域得到广泛应用。

预测编码的基本思想是：利用信号的时域和空域相关性，**利用过去的样本预测未来的样本，并用预测误差来编码**。预测编码一般分为无损编码和有损编码两种。无损编码只考虑信号的精确性，预测误差需要用尽可能少的比特来表示；有损编码则允许一定的失真，可以用较少的比特来表示预测误差，从而达到更高的压缩率。

预测编码有许多不同的实现方法，例如差分编码、基于运动估计的编码、自适应预测编码等。其中，基于运动估计的编码是最常用的预测编码方法之一，它通过对视频中的运动进行建模来实现高效的视频压缩。

比较好的一种理解方法是，参考算法中的前缀和一类问题，预测编码的这种相关性思路与前缀和类问题思路是很相似的。



### 变化(编码)

变换编码是一种信号压缩技术，它将信号从**空域转换到频域**，通过**保留高能量系数，舍弃低能量系数，实现信号的压缩和降噪**。

在变换编码中，通常使用的变换方法有离散余弦变换（**DCT**）和小波变换等。其中，离散余弦变换是最常用的变换方法之一，它可以将信号从空域转换到频域，得到一组频域系数。这些频域系数表示了信号在不同频率上的能量分布，其中低频部分是能量集中的部分，对应着信号的重要信息，高频是能量低的部分，对应着信号中的噪声和冗余信息。

> 注：**高频部分表示的是变化速度较快的信号分量**，即信号中的高频成分。这些高频成分通常代表了信号的细节和纹理信息，例如一张图像中的纹理、边缘、噪声等。而**低频部分表示的是变化速度较慢的信号分量**，即信号中的低频成分，通常代表了信号的整体结构和平滑部分。

接下来，对于离散余弦变换来说，通常需要将信号分割为若干个块，然后对每个块进行变换编码。例如，在图像压缩中，可以将图像分割成若干个 8x8 的块，对每个块进行离散余弦变换，并保留其中的高能量系数，舍弃低能量系数，从而实现图像的压缩。

最后，在变换编码的基础上，通常还需要使用熵编码技术对编码后的数据进行进一步压缩，例如使用哈夫曼编码或算术编码等。这样可以进一步提高压缩比例，从而实现更加高效的信号压缩。

关于DCT，一个比较好的例子是JEPG图片压缩，参考 [【中英双字】JPEG算法原理 jpeg图片是如何压缩的？](https://www.bilibili.com/video/BV1TZ4y1S7iG/)





## DTS PTS

PTS和DTS**都是时间戳**（Timestamp）的一种，用于在视频和音频中确定帧的展示和播放时间。**PTS是Presentation Time Stamp**，表示该帧应该在**播放器上显示的时间戳**，即packet解码后数据的**显示时间**。而DTS是**Decoding Time Stamp**，表示该帧在**解码器中的时间戳**，即packet**解码时间**。

通常情况下，视频编码器会根据帧率等参数给视频帧设置PTS，音频编码器会根据采样率等参数给音频帧设置DTS。在播放时，播放器会根据PTS和DTS来确定音视频帧的播放顺序和时间，保证音视频同步。

需要注意的是，在一些特殊情况下，PTS和DTS可能会出现不同步的情况，比如当视频经过了加速或者放慢处理的时候，或者在一些错误的编码情况下。这时需要进行相应的处理来保证音视频同步。

在音频中，每个采样点都有一个对应的时间戳，因此PTS和DTS在音频中通常是一致的。但**在视频中，因为B帧的存在，PTS和DTS的区别非常明显**。在视频编码中，B帧通常引入了时间的冗余性，导致其在时间轴上的位置不同于原始采集的顺序。因此，在解码过程中，必须根据PTS和DTS对B帧进行重新排序，以保证正确的顺序进行解码。





## 音视频同步问题

播放器单独播放视频流，或者音频流的时候，是不需要 PTS 时间戳的，视频流按帧率播放，音频流按采样率播放即可。这个 PTS 可以帮助音视频同步。

在实际的音视频应用中，由于编码、传输、解码等环节中的不确定性**，音视频不同步的情况才是常态**。例如，在网络环境不稳定的情况下，视频的帧率可能会发生变化，从而导致与之对应的音频与之不同步；或者在解码过程中，由于硬件或软件等原因，可能会出现音视频不同步的情况。此外，一些应用场景下，例如游戏、电视直播等，也需要更高的音视频同步性能，因此在这些场景下，需要更加精细的同步算法和优化手段来保证音视频的同步。

下面来解释第二种情况音视频不同步的原因：因为我们使用的操作系统大多都是**分时系统**，也就是每个任务分配一定的CPU时间片。

假设 Windows 系统正在播放一个视频（没有音频流），帧率是 1秒 24帧，现在是晚上 8点00分00秒。第一帧视频是从 8:00:00:00 开始播放，按帧率播放，41 毫秒就需要显示第二帧，所以第二帧应该在 8:00:00:41 的时候播放，第三帧在 8:00:00:82 的时候播放，这样画面看起来才是流畅的。但是由于是分时系统，在 8:00:00:41 的时候，CPU 在忙着干其他的任务，无法切换回来播放器线程。所以第二帧有可能是在 在 8:00:00:61 的时候才开始播放，慢了 20 毫秒。这种情况我们能怎么办？即使播放器是我们开发的，我们也什么都干不了，在某个时刻 CPU 就是忙不过来。第二帧慢了 20 毫秒，第三帧也慢20毫秒就行了。这样后面CPU不忙碌，看起来也会很流畅。这是单个视频流的情况。

假设有 一个音频流 与 一个视频流 同时播放，本来画面声音是需要同步的，例如第二帧视频播放的时候，第二帧音频也要播放。但是因为视频流慢了 20 毫秒，如果音频流的播放线程不理会 视频播放线程的卡顿情况，音频流还是按自己的采样率播放，就会导致音频流播放快于 视频流 20 毫秒，这样不断累计，音视频流就会逐渐差距很大，画面不同步。所以需要封装格式给 每个 音视频帧打上一个 PTS。音视频流 播放线程 通过观察对方 已经播放的 PTS 来决定自己要继续播放还是休眠，还是丢弃帧。



可采取的措施：

1. **时间戳同步**：视频解码器解码出一帧视频时，将该帧对应的时间戳与音频解码器解码出的对应时间戳进行比较，如果两个时间戳相差很小，则代表音视频同步。如果**相差较大，则需要对其中一个流进行暂停或调整播放速度来保证同步**。
2. 音视频数据的时基同步：音视频文件在封装时，需要将音频和视频的时间戳都转换为一个公共的时间基准，比如说以毫秒为单位。然后，在播放过程中，再根据这个公共时间基准来计算音视频的时间戳，以保证同步。
3. 播放器缓冲控制：播放器缓冲控制也是保证音视频同步的一个重要因素。当播放器缓冲区中的数据足够时，再开始播放，可以保证音视频同步。如果播放器缓冲不足，导致视频数据播放延迟，那么就需要适当的调整音频的播放速度，以保证同步。
4. 码率控制：视频的码率和音频的码率也会影响音视频同步，当视频的码率较高，而音频的码率较低时，可能会导致音视频不同步。在进行视频编码时，需要控制视频的码率，以便与音频的码率相匹配，从而保证音视频同步。





## H.264网络友好结构

H.264的网络友好结构是通过引入一个自适应性的码流控制机制来实现的，该机制被称为“网络抖动缓冲区控制”（Network Abstraction Layer (NAL) Unit Stream Buffering）。

在H.264的码流中，数据被分为多个NAL单元进行传输。每个NAL单元都包含一个单独的码流数据单元，如一帧图像或一组宏块（macroblock）。网络友好结构通过将NAL单元拆分为更小的数据包来减小网络传输时的抖动，并提供了一个自适应性的码流控制机制，以适应不同的网络带宽和延迟。

具体来说，网络友好结构引入了一个抖动缓冲区来缓存接收到的NAL单元，以便在网络拥塞或丢包时提供可靠的播放服务。该抖动缓冲区通过计算NAL单元的时间戳和解码延迟来控制播放速度，并动态地调整NAL单元的传输速率以适应当前的网络状况。

此外，H.264的网络友好结构还提供了多个码率控制模式，如恒定码率（Constant Bitrate，CBR）模式、可变码率（Variable Bitrate，VBR）模式和可变码率多段（Variable Bitrate Multi-Segment，VBR-MS）模式等，以适应不同的应用场景和网络环境。

通过这些网络友好的设计，H.264可以在不同的网络环境下实现更加流畅和高效的视频传输。





## NALU

NALU（Network Abstraction Layer Unit）是视频编码标准H.264/AVC中的一个重要概念，用于描述**一系列网络抽象层单元**。NALU是H.264码流的基本单位，其目的是**将视频数据按照一定的格式和规则进行打包和传输，便于接收端进行解码和渲染**。NALU可以包含各种类型的数据，如视频片段、音频片段、扩展数据等。





## SPS PPS SEI 

SPS、PPS、SEI和Slice是视频编码标准H.264/AVC中的参数。

SPS（**Sequence Parameter Set**）序列参数集，描述了**一系列图像的基本属性，如图像的大小、帧率、码率等**。

PPS（**Picture Parameter Set**）图像参数集，描述了**编码图像的一些具体参数，如帧类型、参考帧的选择等**。

SEI（**Supplemental Enhancement Information**）补充增强信息，描述了**视频流中携带一些额外的信息，如时间戳、水印、字幕、附加数据等。**

Slice（切片？），是**将视频帧切成多个小块进行编码，以提高编码效率和减小延时**。每个Slice包含一部分图像数据，可以单独进行解码和渲染。通过将一帧图像分成多个Slice，可以提高编码效率和视频质量，同时也可以使编码后的视频流在网络传输过程中更容易进行分割和重组。每个Slice都有一个唯一的标识符，通常是它在帧中的位置。Slice的数量和大小取决于所选的编码参数和帧内容。在H.264和H.265等标准中，Slice的数量和大小对于视频质量和压缩效率有很大的影响，因此它们是视频编码中非常重要的概念。



## ffplay SDL

**ffplay 调用 SDL 库来进行音视频播放。**

SDL（Simple DirectMedia Layer）是一个跨平台的开源多媒体库，它**提供了访问视频、音频、输入设备、定时器等硬件资源的简单方法**。SDL 最初是为游戏开发设计的，但也被广泛用于其它领域，如多媒体应用、图形用户界面等。

SDL 的主要特点包括：

- 跨平台性：支持多种操作系统，包括 Windows、Mac OS X、Linux、iOS、Android 等。
- 应用广泛：可以用于游戏、多媒体应用、图形界面等。
- 简单易用：提供了简单的接口和函数，易于使用和学习。
- 支持多种硬件：可以访问音频、视频、输入设备、定时器等硬件资源。
- 开源免费：基于 LGPL 协议发布，可免费使用和修改。

因此，SDL 被广泛应用于多媒体处理和播放领域，包括 ffplay 等音视频播放器。





## DRM

DRM（数字版权管理）是一种技术，用于控制数字媒体内容在数字环境中的使用。它通常被用于保护受版权保护的内容，以防止盗版和非法共享。DRM使用加密和许可证技术来控制数字媒体内容的访问和使用，可以限制内容的拷贝、打印、转码和分发等功能。这些技术通常用于数字媒体内容，如音乐、电影、电子书、软件和游戏等。

DRM技术包括多种不同的方案和实现方式，例如数字签名、加密、许可证、数字水印等。DRM系统通常由硬件和软件组成，其中硬件组件包括数字信号处理器（DSP）、加密芯片、许可证芯片等，软件组件包括应用程序、驱动程序、系统库等。DRM技术的实现和应用对数字内容的保护和共享有着重要的作用。







## 沙盒目录

沙盒目录（sandbox directory）是指**操作系统为了隔离不同应用程序的文件系统访问而设置的一种机制**。在沙盒目录中，每个应用程序都只能访问自己的特定目录和文件，不能访问其他应用程序的文件。这样可以提高应用程序的安全性，防止应用程序因访问其他应用程序的敏感数据而导致的安全漏洞。沙盒目录通常用于移动设备、桌面应用程序和浏览器等领域。





## 端序

大端序（Big Endian）和小端序（Little Endian）是两种计算机存储数据的字节序（Byte Order）方式。字节序指的是在内存中多字节数据（例如整数或浮点数）的存储顺序，特别是指高位字节和低位字节的存放顺序。

1. 大端序（Big Endian）：
   在大端序中，高位字节存储在低地址，低位字节存储在高地址。这意味着在多字节数据中，最高有效字节（MSB）位于最低的内存地址，而最低有效字节（LSB）位于最高的内存地址。



2. 小端序（Little Endian）：
   在小端序中，高位字节存储在高地址，低位字节存储在低地址。这意味着在多字节数据中，最高有效字节（MSB）位于最高的内存地址，而最低有效字节（LSB）位于最低的内存地址。



在现代计算机体系结构中，不同的处理器可能采用不同的字节序。例如，x86架构的CPU通常采用小端序，而**大部分网络传输和存储系统采用大端序**。因此，在进行数据交换和跨平台通信时，需要注意字节序的转换，以确保数据的正确传输和解析。

> 举个例子：
>
> 对于十进制数值 23456，其在十六进制表示为 0x5BB0。现在我们将它按照大端序和小端序分别表示：
>
> 1. 大端序（Big Endian）表示：
>    在大端序中，高位字节存储在低地址，低位字节存储在高地址。
>
> 大端序表示： 0x5B B0
>
> 2. 小端序（Little Endian）表示：
>    在小端序中，高位字节存储在高地址，低位字节存储在低地址。
>
> 小端序表示： 0xB0 5B
>
> 在多字节数据存储时，大端序和小端序的区别就会显现出来，即高位字节和低位字节在内存中的存储顺序不同。



